{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283a50f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hybrid with CF: final score = 0.7*embed_cos + 0.3*CF_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0ea0c",
   "metadata": {},
   "source": [
    "## Assemble a DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13091878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "groups = pd.read_parquet(\n",
    "    \"/workspace/data/processed/groups_for_recs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2afd651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 82 rows with priceSEK < 50\n"
     ]
    }
   ],
   "source": [
    "groups['priceSEK'] = pd.to_numeric(groups['priceSEK'], errors='coerce')\n",
    "before_count = len(groups)\n",
    "groups = groups[groups['priceSEK'] >= 50]\n",
    "after_count = len(groups)\n",
    "print(f\"Dropped {before_count - after_count} rows with priceSEK < 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82550ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price buckets based on the distribution of price_sek, using 6 buckets\n",
    "price_bins = [0, 100, 300, 600, 1000, 2000, float('inf')]\n",
    "price_labels = [\n",
    "    'Budget',        # 0-100\n",
    "    'Value',         # 100-300\n",
    "    'Popular',       # 300-600\n",
    "    'Premium',       # 600-1000\n",
    "    'Luxury',        # 1000-2000\n",
    "    'Exclusive'      # 2000+\n",
    "]\n",
    "groups['priceband'] = pd.cut(groups['priceSEK'], bins=price_bins, labels=price_labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441943d8",
   "metadata": {},
   "source": [
    "## 1. Build a clean text field for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1546840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, unicodedata, re, numpy as np\n",
    "\n",
    "MISSING = {\"\", \"unknown\", \"nan\", \"none\", None}\n",
    "\n",
    "def canon(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = re.sub(r\"\\u00A0\", \" \", s)\n",
    "    s = re.sub(r\"[\\u2010-\\u2015\\u2212\\-]+\", \"-\", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def norm_categories(x):\n",
    "    cats = [canon(c) for c in str(x).split(\",\") if str(c).strip() not in MISSING]\n",
    "    seen, out = set(), []\n",
    "    for c in cats:\n",
    "        cl = c.lower()\n",
    "        if c and cl not in seen:\n",
    "            seen.add(cl)\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def short_desc(desc, max_words=30):\n",
    "    if not desc: return \"\"\n",
    "    first = re.split(r\"(?<=[.!?])\\s+\", desc)[0]\n",
    "    return \" \".join(first.split()[:max_words])\n",
    "\n",
    "def format_colors(col) -> str:\n",
    "    \"\"\"\n",
    "    Render colors as 'Svart, Grå' (no brackets). Accepts list/tuple/Series/ndarray or strings like:\n",
    "    \"['Svart' 'Grå']\", \"Grå,Svart\", \"Svart/Grå\".\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    if isinstance(col, (list, tuple, pd.Series, np.ndarray)):\n",
    "        seq = list(col)\n",
    "        for v in seq:\n",
    "            s = str(v).strip()\n",
    "            if not s or s.lower() in MISSING: \n",
    "                continue\n",
    "            # split embedded multi-values too (e.g., 'Grå,Svart')\n",
    "            parts = re.split(r\"\\s*[,/|;]\\s*\", s) if any(sep in s for sep in \",/|;\") else [s]\n",
    "            vals.extend(parts)\n",
    "    else:\n",
    "        s = str(col).strip()\n",
    "        if s and s.lower() not in MISSING:\n",
    "            quoted = re.findall(r\"'([^']+)'|\\\"([^\\\"]+)\\\"\", s)\n",
    "            if quoted:\n",
    "                vals = [a or b for a, b in quoted]\n",
    "            else:\n",
    "                vals = re.split(r\"\\s*[,/|;]\\s*\", s) if any(sep in s for sep in \",/|;\") else [s]\n",
    "\n",
    "    # order-preserving dedupe\n",
    "    out, seen = [], set()\n",
    "    for v in vals:\n",
    "        t = v.strip()\n",
    "        if t and t.lower() not in seen:\n",
    "            seen.add(t.lower())\n",
    "            out.append(t)\n",
    "    return \", \".join(out)\n",
    "\n",
    "# ---- build ----\n",
    "groups = groups.copy()\n",
    "\n",
    "# Ensure color column exists\n",
    "if \"color\" not in groups.columns:\n",
    "    groups[\"color\"] = \"\"\n",
    "\n",
    "# Single normalized categories column\n",
    "groups[\"categories\"] = groups[\"category\"].apply(norm_categories)\n",
    "\n",
    "# Nice string rendering of colors for text/metadata\n",
    "groups[\"colors_str\"] = groups[\"color\"].apply(format_colors)\n",
    "\n",
    "def build_text_embed_clean(r):\n",
    "    name  = canon(r.get(\"name\", \"\"))\n",
    "    desc  = short_desc(canon(r.get(\"description\", \"\")), 30)\n",
    "    brand_raw = r.get(\"brand\", \"\")\n",
    "    brand = canon(brand_raw) if str(brand_raw).strip() and str(brand_raw).strip().lower() not in MISSING else \"\"\n",
    "    cats  = r.get(\"categories\", []) or []\n",
    "    cols  = r.get(\"colors_str\", \"\")\n",
    "\n",
    "    parts, attrs = [], []\n",
    "    if name: parts.append(f\"{name}.\")\n",
    "    if desc: parts.append(desc)\n",
    "    if brand: attrs.append(brand)\n",
    "    if cats:  attrs.append(\", \".join(cats))\n",
    "    if cols:  attrs.append(cols)\n",
    "    if attrs: parts.append(\" \".join(attrs) + \".\")\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "\n",
    "groups[\"text\"] = groups.apply(build_text_embed_clean, axis=1)\n",
    "\n",
    "group_df = groups[[\n",
    "    \"groupId\",\n",
    "    \"text\",\n",
    "    \"color\",        # original raw colors (list/string as-is)\n",
    "    \"colors_str\",\n",
    "    \"categories\",\n",
    "    \"brand\",\n",
    "    \"priceband\"\n",
    "]].reset_index(drop=True)\n",
    "\n",
    "# Corpus for embedding\n",
    "corpus = group_df[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a2f812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py exe: /usr/local/bin/python\n",
      "torch: 2.8.0+cpu at /home/vscode/.local/lib/python3.10/site-packages/torch/__init__.py\n",
      "transformers: 4.57.0\n",
      "sentence-transformers: 5.1.1\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, transformers, sentence_transformers\n",
    "print(\"py exe:\", sys.executable)\n",
    "print(\"torch:\", torch.__version__, \"at\", torch.__file__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"sentence-transformers:\", sentence_transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2759204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!python -m pip install --upgrade --index-url https://download.pytorch.org/whl/cpu torch==2.8.0\n",
    "\n",
    "#pip install --no-deps sentence-transformers transformers tokenizers huggingface_hub safetensors\n",
    "#pip install --no-cache-dir \"sentencepiece==0.1.99\"\n",
    "#pip install faiss-cpu scikit-learn\n",
    "\n",
    "#pip install \"huggingface_hub[hf_xet]\"  # or: pip install hf_xet\n",
    "\n",
    "#pip install regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bf17a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 4096, 'do_lower_case': False, 'architecture': 'NewModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#load encoder\n",
    "MODEL_ID = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "\n",
    "import os, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "enc = SentenceTransformer(MODEL_ID, device=\"cpu\", trust_remote_code=True)\n",
    "enc.max_seq_length = min(4096, enc.tokenizer.model_max_length)\n",
    "print(enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33cc25e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1648, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embed texts\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "texts = group_df[\"text\"].fillna(\"\").tolist()\n",
    "\n",
    "E = enc.encode(\n",
    "    texts,\n",
    "    batch_size=512,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=False\n",
    ").astype(\"float32\")\n",
    "\n",
    "N, d = E.shape\n",
    "N, d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4238414d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1648, True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FAISS index (cosine via inner product) + gid map\n",
    "\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(d)   # cosine since E is normalized\n",
    "index.add(E)\n",
    "\n",
    "# fast groupId -> row lookup (store as str to be safe)\n",
    "gid2i = {str(g): i for i, g in enumerate(group_df[\"groupId\"].astype(str))}\n",
    "N, index.is_trained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d51e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precompute neighbors\n",
    "K = 10\n",
    "COS_MIN = 0.60\n",
    "\n",
    "# One batched search for everyone\n",
    "S_all, I_all = index.search(E, K + 1)  # includes self\n",
    "\n",
    "neighbors, sims_list = [], []\n",
    "for i in range(N):\n",
    "    js, ss = I_all[i], S_all[i]\n",
    "    m = (js != i) & (ss >= COS_MIN)     # drop self + cosine cutoff\n",
    "    js, ss = js[m][:K], ss[m][:K]\n",
    "    neighbors.append(js.tolist())\n",
    "    sims_list.append(ss.astype(float).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4c1f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_gid(target_gid):\n",
    "    i = gid2i[str(target_gid)]\n",
    "    js, ss = neighbors[i], sims_list[i]\n",
    "    if not js:\n",
    "        return group_df.iloc[[]].assign(similarity=[]).reset_index(drop=True)\n",
    "    out = group_df.iloc[js].copy()\n",
    "    out[\"similarity\"] = ss\n",
    "    return out.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4cb9f8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>groupId</th>\n",
       "      <th>text</th>\n",
       "      <th>color</th>\n",
       "      <th>colors_str</th>\n",
       "      <th>categories</th>\n",
       "      <th>brand</th>\n",
       "      <th>priceband</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527002</td>\n",
       "      <td>Fyndpaket Heminredning. Nu erbjuder vi dig som...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[REA]</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0.749368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206532</td>\n",
       "      <td>Fyndpaket stor. Fyndpaket. Åshild REA.</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[REA]</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Popular</td>\n",
       "      <td>0.684989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206524</td>\n",
       "      <td>Fyndpaket liten. Fyndpaket. Åshild REA.</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[REA]</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Value</td>\n",
       "      <td>0.672018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536763</td>\n",
       "      <td>Fyndpaket Jul Hemtextil/juldekorationer. Fyndp...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[REA]</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0.649287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350600</td>\n",
       "      <td>Fyndpaket Hobbyhörnan. Överraskningspaket med ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[Hobbyhörnan, Pysselset]</td>\n",
       "      <td>Ateljé Margaretha</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0.601266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  groupId                                               text color colors_str  \\\n",
       "0  527002  Fyndpaket Heminredning. Nu erbjuder vi dig som...    []              \n",
       "1  206532             Fyndpaket stor. Fyndpaket. Åshild REA.    []              \n",
       "2  206524            Fyndpaket liten. Fyndpaket. Åshild REA.    []              \n",
       "3  536763  Fyndpaket Jul Hemtextil/juldekorationer. Fyndp...    []              \n",
       "4  350600  Fyndpaket Hobbyhörnan. Överraskningspaket med ...    []              \n",
       "\n",
       "                 categories              brand priceband  similarity  \n",
       "0                     [REA]            unknown   Premium    0.749368  \n",
       "1                     [REA]             Åshild   Popular    0.684989  \n",
       "2                     [REA]             Åshild     Value    0.672018  \n",
       "3                     [REA]            unknown   Premium    0.649287  \n",
       "4  [Hobbyhörnan, Pysselset]  Ateljé Margaretha   Premium    0.601266  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example:\n",
    "#search_by_gid(\"290281\", k=10)\n",
    "search_by_gid(\"106065\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0328948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wide_topk_parquet(path=\"semantic_similarity_recs.parquet\"):\n",
    "    gids = group_df[\"groupId\"].astype(str).tolist()\n",
    "    rows = []\n",
    "    for i, gid in enumerate(gids):\n",
    "        rec_ids = group_df.iloc[neighbors[i]][\"groupId\"].astype(str).tolist()\n",
    "        rows.append([gid] + rec_ids + [None] * (K - len(rec_ids)))\n",
    "    import pandas as pd\n",
    "    wide = pd.DataFrame(rows, columns=[\"Product ID\"] + [f\"Top {r}\" for r in range(1, K+1)])\n",
    "    wide.to_parquet(path, index=False)\n",
    "    return wide\n",
    "\n",
    "wide = save_wide_topk_parquet()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (itcm-recsys)",
   "language": "python",
   "name": "itcm-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
