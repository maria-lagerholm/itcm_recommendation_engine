{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "258afbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55a41d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.read_parquet(\n",
    "    \"/workspace/data/processed/articles_for_recs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0479d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 18 rows with priceSEK < 1\n"
     ]
    }
   ],
   "source": [
    "groups['priceSEK'] = pd.to_numeric(groups['priceSEK'], errors='coerce')\n",
    "before_count = len(groups)\n",
    "groups = groups[groups['priceSEK'] >= 1]\n",
    "after_count = len(groups)\n",
    "print(f\"Dropped {before_count - after_count} rows with priceSEK < 1\")\n",
    "\n",
    "if 'audienceId' in groups.columns:\n",
    "    groups = groups.drop(columns=['audienceId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae943454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np   # <-- add this if you use np.ndarray\n",
    "\n",
    "MISSING = {\"\", \"unknown\", \"nan\", \"none\", None}\n",
    "\n",
    "def canon(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = re.sub(r\"\\u00A0\", \" \", s)\n",
    "    s = re.sub(r\"[\\u2010-\\u2015\\u2212\\-]+\", \"-\", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# --- keep your existing norm_categories, short_desc, and format_colors ---\n",
    "\n",
    "def format_sizes(sz) -> str:\n",
    "    \"\"\"\n",
    "    Render sizes as '36/38, 40/42' (no brackets).\n",
    "    Accepts list/tuple/Series/ndarray or strings like:\n",
    "    \"['36/38' '40/42']\", \"36/38,40/42\", \"36/38 | 40/42\".\n",
    "    NOTE: We DO NOT split on '/' because '36/38' is one size token.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "\n",
    "    # sequence-like input\n",
    "    if isinstance(sz, (list, tuple, pd.Series, np.ndarray)):\n",
    "        for v in list(sz):\n",
    "            s = str(v).strip()\n",
    "            if not s or s.lower() in MISSING:\n",
    "                continue\n",
    "            # split only on comma/pipe/semicolon, NOT on '/'\n",
    "            parts = re.split(r\"\\s*[,|;]\\s*\", s) if any(sep in s for sep in \",|;\") else [s]\n",
    "            vals.extend(parts)\n",
    "\n",
    "    else:\n",
    "        s = str(sz).strip()\n",
    "        if s and s.lower() not in MISSING:\n",
    "            # strings like \"['36/38' '40/42']\" or '[\"36/38\",\"40/42\"]'\n",
    "            quoted = re.findall(r\"'([^']+)'|\\\"([^\\\"]+)\\\"\", s)\n",
    "            if quoted:\n",
    "                vals = [a or b for a, b in quoted]\n",
    "            else:\n",
    "                vals = re.split(r\"\\s*[,|;]\\s*\", s) if any(sep in s for sep in \",|;\") else [s]\n",
    "\n",
    "    # order-preserving dedupe with normalization\n",
    "    out, seen = [], set()\n",
    "    for v in vals:\n",
    "        t = canon(v)\n",
    "        if t and t.lower() not in seen and t.lower() not in MISSING:\n",
    "            seen.add(t.lower())\n",
    "            out.append(t)\n",
    "    return \", \".join(out)\n",
    "\n",
    "# ---- build ----\n",
    "groups = groups.copy()\n",
    "\n",
    "# Ensure columns exist\n",
    "if \"color\" not in groups.columns:\n",
    "    groups[\"color\"] = \"\"\n",
    "if \"size\" not in groups.columns:\n",
    "    groups[\"size\"] = \"\"   # <-- add size\n",
    "\n",
    "# Normalized categories\n",
    "groups[\"categories\"] = groups[\"category\"].apply(norm_categories)\n",
    "\n",
    "# Nice string renderings\n",
    "groups[\"colors_str\"] = groups[\"color\"].apply(format_colors)\n",
    "groups[\"sizes_str\"]  = groups[\"size\"].apply(format_sizes)   # <-- add sizes\n",
    "\n",
    "def build_text_embed_clean(r):\n",
    "    name  = canon(r.get(\"name\", \"\"))\n",
    "    desc  = short_desc(canon(r.get(\"description\", \"\")), 30)\n",
    "    brand = canon(r.get(\"brand\", \"\"))\n",
    "    cats  = r.get(\"categories\", []) or []\n",
    "    cols  = r.get(\"colors_str\", \"\")\n",
    "    sizes = r.get(\"sizes_str\", \"\")   # <-- add sizes\n",
    "\n",
    "    aud   = canon(r.get(\"audience\", \"\"))\n",
    "\n",
    "    parts, attrs = [], []\n",
    "    parts.append(f\"AUDIENCE: {aud}.\")\n",
    "\n",
    "    if name:  parts.append(f\"{name}.\")\n",
    "    if desc:  parts.append(desc)\n",
    "    if brand: attrs.append(brand)\n",
    "    if cats:  attrs.append(\", \".join(cats))\n",
    "    if cols:  attrs.append(cols)\n",
    "    if sizes: attrs.append(sizes)    # <-- include sizes\n",
    "    if attrs: parts.append(\" \".join(attrs) + \".\")\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "\n",
    "groups[\"text\"] = groups.apply(build_text_embed_clean, axis=1)\n",
    "\n",
    "group_df = groups[[\n",
    "    \"groupId\",\n",
    "    \"text\",\n",
    "    \"audience\",\n",
    "    \"color\",\n",
    "    \"colors_str\",\n",
    "    \"size\",         # <-- include raw size if you want it downstream\n",
    "    \"sizes_str\",    # <-- include formatted sizes\n",
    "    \"categories\",\n",
    "    \"brand\",\n",
    "    \"name\"\n",
    "]].reset_index(drop=True)\n",
    "\n",
    "corpus = group_df[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e42b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 4096, 'do_lower_case': False, 'architecture': 'NewModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "\n",
    "import os, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "enc = SentenceTransformer(MODEL_ID, device=\"cpu\", trust_remote_code=True)\n",
    "enc.max_seq_length = min(4096, enc.tokenizer.model_max_length)\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb066d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525b306c17434aca9bbc16702b8d2cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#embed texts\n",
    "\n",
    "texts = group_df[\"text\"].fillna(\"\").tolist()\n",
    "\n",
    "E = enc.encode(\n",
    "    texts,\n",
    "    batch_size=512,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "N, d = E.shape\n",
    "N, d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4c4cf",
   "metadata": {},
   "source": [
    "Here the change from semantic search begins: to reuse the index for searching by product ID later\n",
    "replace IndexFlatIP with an ID-mapped index so you can look up items by their real IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd3bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexIDMap2(faiss.IndexFlatIP(d))  # cosine since E is normalized\n",
    "ids = group_df[\"groupId\"].astype(np.int64).to_numpy()\n",
    "index.add_with_ids(E, ids)\n",
    "\n",
    "N, index.is_trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec123af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index to disk\n",
    "faiss.write_index(index, \"products.faiss\")\n",
    "\n",
    "# Save metadata for quick lookup\n",
    "group_df[[\"groupId\", \"name\", \"brand\", \"categories\", \"audience\", \"color\", \"size\", \"text\"]].to_parquet(\n",
    "    \"product_meta.parquet\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e843e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brands\n",
    "unique_brands = pd.Series(group_df[\"brand\"].dropna().unique())\n",
    "unique_brands_lower = unique_brands.str.lower().unique()\n",
    "pd.Series(unique_brands_lower).to_frame(name=\"brand\").to_parquet(\"brands.parquet\", index=False)\n",
    "\n",
    "# Colors\n",
    "# Flatten the color lists, remove NAs, and get unique (case-insensitive) color names\n",
    "all_colors = group_df[\"color\"].dropna().explode()\n",
    "unique_colors = pd.Series(all_colors.dropna().unique())\n",
    "unique_colors_lower = unique_colors.str.lower().unique()\n",
    "pd.Series(unique_colors_lower).to_frame(name=\"color\").to_parquet(\"colors.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be18be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb10b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>groupId</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>score</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210730</td>\n",
       "      <td>Rutig blus</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>0.717682</td>\n",
       "      <td>[Blå]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219477</td>\n",
       "      <td>Herrskjorta</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.690796</td>\n",
       "      <td>[Blå, Röd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>210247</td>\n",
       "      <td>Kortärmad bomullsblus</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>0.688460</td>\n",
       "      <td>[Vit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270312</td>\n",
       "      <td>Herrpyjamas blå/vit</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>0.671869</td>\n",
       "      <td>[Blå, Marin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>507863</td>\n",
       "      <td>Bomullslakan</td>\n",
       "      <td>Borganäs of Sweden</td>\n",
       "      <td>0.664199</td>\n",
       "      <td>[Blå, Brun, Grön, Linne, Ljusblå, Ljusgrå, Mör...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>210748</td>\n",
       "      <td>Blus</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>0.656180</td>\n",
       "      <td>[Svart]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>290039</td>\n",
       "      <td>G-punktsvibrator Bodil blå Belladot</td>\n",
       "      <td>Belladot</td>\n",
       "      <td>0.654369</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>270095</td>\n",
       "      <td>Tankini</td>\n",
       "      <td>Damella</td>\n",
       "      <td>0.651817</td>\n",
       "      <td>[Blå]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>242511</td>\n",
       "      <td>Bomullsbyxa Julia</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>0.649073</td>\n",
       "      <td>[Beige, Ljusblå, Marin, Vit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>531400</td>\n",
       "      <td>Bågkappa Fanny med broderade blommor</td>\n",
       "      <td>Linea</td>\n",
       "      <td>0.648997</td>\n",
       "      <td>[Blå, Vit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   groupId                                  name               brand  \\\n",
       "0   210730                           Rutig blus               Åshild   \n",
       "1   219477                           Herrskjorta             unknown   \n",
       "2   210247                 Kortärmad bomullsblus              Åshild   \n",
       "3   270312                   Herrpyjamas blå/vit              Åshild   \n",
       "4   507863                          Bomullslakan  Borganäs of Sweden   \n",
       "5   210748                                 Blus               Åshild   \n",
       "6   290039   G-punktsvibrator Bodil blå Belladot            Belladot   \n",
       "7   270095                               Tankini             Damella   \n",
       "8   242511                    Bomullsbyxa Julia               Åshild   \n",
       "9   531400  Bågkappa Fanny med broderade blommor               Linea   \n",
       "\n",
       "      score                                              color  \n",
       "0  0.717682                                              [Blå]  \n",
       "1  0.690796                                         [Blå, Röd]  \n",
       "2  0.688460                                              [Vit]  \n",
       "3  0.671869                                       [Blå, Marin]  \n",
       "4  0.664199  [Blå, Brun, Grön, Linne, Ljusblå, Ljusgrå, Mör...  \n",
       "5  0.656180                                            [Svart]  \n",
       "6  0.654369                                                 []  \n",
       "7  0.651817                                              [Blå]  \n",
       "8  0.649073                       [Beige, Ljusblå, Marin, Vit]  \n",
       "9  0.648997                                         [Blå, Vit]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load index\n",
    "index = faiss.read_index(\"products.faiss\")\n",
    "\n",
    "# Encode a query (e.g., user's search text)\n",
    "query = \"blå M\"\n",
    "qv = enc.encode([query], normalize_embeddings=True, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "# Search top 10 similar items\n",
    "scores, ids = index.search(qv, 10)\n",
    "\n",
    "\n",
    "meta = pd.read_parquet(\"product_meta.parquet\")\n",
    "\n",
    "\n",
    "meta = meta.copy()\n",
    "meta[\"groupId\"] = meta[\"groupId\"].astype(np.int64)\n",
    "\n",
    "# Build ranking dataframe from FAISS results and join names\n",
    "rank = pd.DataFrame({\"groupId\": ids[0].astype(np.int64), \"score\": scores[0]})\n",
    "rank = rank[rank[\"groupId\"] != -1]\n",
    "\n",
    "out = (rank\n",
    "       .merge(meta[[\"groupId\", \"name\", \"brand\", \"color\", \"size\"]], on=\"groupId\", how=\"left\")\n",
    "       .sort_values(\"score\", ascending=False)\n",
    "       .reset_index(drop=True))\n",
    "\n",
    "out[[\"groupId\", \"name\", \"brand\", \"score\", \"color\", \"size\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rerank FAISS candidates on CPU (minimal) ---\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch, pandas as pd, numpy as np\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-reranker-base\")\n",
    "rerank = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Alibaba-NLP/gte-multilingual-reranker-base\", trust_remote_code=True\n",
    ").eval()  # CPU fp32\n",
    "\n",
    "# 2) Retrieve with FAISS\n",
    "K = 100\n",
    "scores, ids = index.search(qv, K)\n",
    "cand_ids = ids[0].astype(np.int64)\n",
    "cand_ids = cand_ids[cand_ids != -1]\n",
    "\n",
    "# 3) Prepare texts for reranking\n",
    "meta = pd.read_parquet(\"product_meta.parquet\").copy()\n",
    "meta[\"groupId\"] = meta[\"groupId\"].astype(np.int64)\n",
    "\n",
    "\n",
    "cands = (pd.DataFrame({\"groupId\": cand_ids})\n",
    "         .merge(meta[[\"groupId\",\"name\",\"brand\",\"color\",\"size\",\"text\"]], on=\"groupId\", how=\"left\")\n",
    "         .dropna(subset=[\"text\"]))\n",
    "\n",
    "# 4) Cross-encode (query, doc) pairs and rerank\n",
    "pairs = [(query, t) for t in cands[\"text\"].tolist()]\n",
    "with torch.no_grad():\n",
    "    batch = tok(pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    ce = rerank(**batch).logits.view(-1).numpy()\n",
    "\n",
    "cands[\"rerank_score\"] = ce\n",
    "final = (cands.sort_values(\"rerank_score\", ascending=False)\n",
    "         .loc[:, [\"groupId\",\"name\",\"brand\",\"color\",\"size\",\"rerank_score\"]]\n",
    "         .reset_index(drop=True))\n",
    "\n",
    "print(final.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (itcm-recsys)",
   "language": "python",
   "name": "itcm-recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
