{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4053143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Low-CPU mode\n",
    "#import os\n",
    "#for k in (\"ARROW_NUM_THREADS\", \"OMP_NUM_THREADS\", \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_MAX_THREADS\"):\n",
    "#    os.environ.setdefault(k, \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fb56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tx = pd.read_parquet(\"../data/processed/transactions_clean.parquet\")\n",
    "#customers = pd.read_parquet(\"../data/processed/customers_clean.parquet\")\n",
    "articles  = pd.read_parquet(\"../data/processed/articles_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3c6193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku                         object\n",
       "groupId                     object\n",
       "brandId             string[python]\n",
       "brand_missing                 int8\n",
       "name                        object\n",
       "description         string[python]\n",
       "brand               string[python]\n",
       "color               string[python]\n",
       "colorId             string[python]\n",
       "size                string[python]\n",
       "size_missing                  int8\n",
       "sizeId              string[python]\n",
       "audience            string[python]\n",
       "audienceId          string[python]\n",
       "audience_missing              int8\n",
       "category            string[python]\n",
       "category_missing              int8\n",
       "categoryId          string[python]\n",
       "priceSEK            string[python]\n",
       "priceEUR            string[python]\n",
       "priceNOK            string[python]\n",
       "priceDKK            string[python]\n",
       "quantity            string[python]\n",
       "publishedDate       string[python]\n",
       "forSale             string[python]\n",
       "fabrics                     object\n",
       "fabric_primary              object\n",
       "fabrics_en                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc6a1121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orderId                0\n",
       "shopUserId             0\n",
       "created                0\n",
       "currencyId             0\n",
       "sku                    0\n",
       "groupId                0\n",
       "quantity               0\n",
       "price                  0\n",
       "name                   0\n",
       "type                   0\n",
       "invoiceCity            0\n",
       "category               0\n",
       "brand                  0\n",
       "sek_rate               0\n",
       "price_sek              0\n",
       "Age               104713\n",
       "Gender            104713\n",
       "line_total_sek         0\n",
       "country                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e76633",
   "metadata": {},
   "source": [
    "## Build json for mental mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66547eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Setup & Constants\n",
    "# Lightweight imports and shared constants.\n",
    "\n",
    "# %%\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "NORDICS = [\"Sweden\", \"Denmark\", \"Finland\", \"Norway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbeea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Generic helpers\n",
    "# Small utilities reused across the pipeline.\n",
    "\n",
    "# %%\n",
    "def status_from_orders(n: int) -> str:\n",
    "    return \"new\" if n <= 1 else (\"returning\" if n <= 3 else \"loyal\")\n",
    "\n",
    "def mode_or_first(s: pd.Series):\n",
    "    m = s.mode()\n",
    "    if not m.empty:\n",
    "        return m.iat[0]\n",
    "    s = s.dropna()\n",
    "    return s.iat[0] if not s.empty else None\n",
    "\n",
    "def _pick(tx: pd.DataFrame, names, default=None) -> pd.Series:\n",
    "    for n in names:\n",
    "        if n in tx.columns:\n",
    "            return tx[n]\n",
    "    return pd.Series([default] * len(tx), index=tx.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f454321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Normalization helpers\n",
    "# Standardize ids and small categorical fields.\n",
    "\n",
    "# %%\n",
    "def _norm_id(s: pd.Series) -> pd.Series:\n",
    "    # to string, strip, drop trailing \".0\" if present (common after Parquet/float cast)\n",
    "    s = s.astype(\"string[python]\").str.strip()\n",
    "    return s.str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "\n",
    "def _norm_gender(s: pd.Series):\n",
    "    if s is None:\n",
    "        return s\n",
    "    return s.astype(\"string[python]\").str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40c4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Tx base-frame builder\n",
    "# Make a clean base transaction frame with unified schema.\n",
    "\n",
    "# %%\n",
    "def _build_base_tx(tx: pd.DataFrame) -> pd.DataFrame:\n",
    "    country = _pick(tx, [\"currency_country\", \"Country\"], \"Unknown\").astype(\"string[python]\").str.strip()\n",
    "    country = country.replace({\"\": pd.NA}).fillna(\"Unknown\")\n",
    "    city = _pick(tx, [\"invoiceCity\", \"city\"], \"Unknown\").astype(object).fillna(\"Unknown\")\n",
    "    shop = _norm_id(tx[\"shopUserId\"])\n",
    "    order_id = tx[\"orderId\"].astype(\"string[python]\").str.strip()\n",
    "\n",
    "    created_raw = tx[\"created\"]\n",
    "    created = created_raw if np.issubdtype(created_raw.dtype, np.datetime64) else pd.to_datetime(created_raw, errors=\"coerce\")\n",
    "    if isinstance(created, pd.Series) and np.issubdtype(created.dtype, np.datetime64):\n",
    "        try:\n",
    "            if getattr(created.dt, \"tz\", None) is not None:\n",
    "                created = created.dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    rev = pd.to_numeric(tx.get(\"line_total_sek\"), errors=\"coerce\").fillna(0)\n",
    "    typ = tx[\"type\"] if \"type\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "    price = tx[\"price\"] if \"price\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"shopUserId\": shop,\n",
    "            \"orderId\": order_id,\n",
    "            \"rev\": rev,\n",
    "            \"created\": created,\n",
    "            \"type\": typ,\n",
    "            \"price\": price,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Tx base-frame builder\n",
    "# Make a clean base transaction frame with unified schema.\n",
    "\n",
    "# %%\n",
    "def _build_base_tx(tx: pd.DataFrame) -> pd.DataFrame:\n",
    "    # country & city (no defaulting if you know they exist)\n",
    "    country = tx[\"country\"].astype(\"string[python]\").str.strip()\n",
    "    city = _pick(tx, [\"invoiceCity\", \"city\"], \"Unknown\").astype(object)\n",
    "\n",
    "    # ids\n",
    "    shop = _norm_id(tx[\"shopUserId\"])\n",
    "    order_id = tx[\"orderId\"].astype(\"string[python]\").str.strip()\n",
    "\n",
    "    # dates\n",
    "    created_raw = tx[\"created\"]\n",
    "    created = created_raw if np.issubdtype(created_raw.dtype, np.datetime64) else pd.to_datetime(created_raw, errors=\"coerce\")\n",
    "    if isinstance(created, pd.Series) and np.issubdtype(created.dtype, np.datetime64):\n",
    "        try:\n",
    "            if getattr(created.dt, \"tz\", None) is not None:\n",
    "                created = created.dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # money & misc\n",
    "    rev = pd.to_numeric(tx.get(\"line_total_sek\"), errors=\"coerce\").fillna(0)\n",
    "    typ = tx[\"type\"] if \"type\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "    price = tx[\"price\"] if \"price\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "\n",
    "    # NEW: carry Age / Gender from tx\n",
    "    age = pd.to_numeric(tx.get(\"Age\"), errors=\"coerce\").astype(\"Float64\") if \"Age\" in tx.columns else pd.Series([pd.NA] * len(tx), index=tx.index, dtype=\"Float64\")\n",
    "    gender = tx.get(\"Gender\") if \"Gender\" in tx.columns else pd.Series([pd.NA] * len(tx), index=tx.index, dtype=\"object\")\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"shopUserId\": shop,\n",
    "            \"orderId\": order_id,\n",
    "            \"rev\": rev,\n",
    "            \"created\": created,\n",
    "            \"type\": typ,\n",
    "            \"price\": price,\n",
    "            \"Age\": age,          # <- keep case exactly as used downstream\n",
    "            \"Gender\": gender,    # <- keep case exactly as used downstream\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cadbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Customer normalization & collapse\n",
    "# Normalize ids, pick Age/Gender variants, and collapse duplicates.\n",
    "\n",
    "# %%\n",
    "def _prep_customers(customers: pd.DataFrame) -> pd.DataFrame:\n",
    "    c = customers.copy()\n",
    "    c[\"shopUserId_norm\"] = _norm_id(c[\"shopUserId\"])\n",
    "    c[\"Age\"] = pd.to_numeric(_pick(c, [\"Age\", \"age\", \"customer_age\"]), errors=\"coerce\").astype(\"Float64\")\n",
    "    c[\"Gender\"] = _pick(c, [\"Gender\", \"gender\", \"customer_gender\"])\n",
    "    c_agg = (\n",
    "        c.groupby(\"shopUserId_norm\", dropna=False)\n",
    "        .agg(Age=(\"Age\", mode_or_first), Gender=(\"Gender\", mode_or_first))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return c_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acd90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Customer normalization & collapse\n",
    "# Normalize ids, pick Age/Gender variants, and collapse duplicates.\n",
    "\n",
    "# %%\n",
    "def _prep_customers(customers: pd.DataFrame) -> pd.DataFrame:\n",
    "    c = customers.copy()\n",
    "    c[\"shopUserId_norm\"] = _norm_id(c[\"shopUserId\"])\n",
    "    c_agg = (\n",
    "        c.groupby(\"shopUserId_norm\", dropna=False)\n",
    "        .agg(Age=(\"Age\", mode_or_first), Gender=(\"Gender\", mode_or_first))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return c_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77fe78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORDICS = [\"Sweden\", \"Denmark\", \"Finland\", \"Norway\"]\n",
    "\n",
    "def split_nordics(tx: pd.DataFrame) -> dict:\n",
    "    base_tx = _build_base_tx(tx)\n",
    "    return {cname: base_tx[base_tx[\"country\"] == cname].copy() for cname in NORDICS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da06a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export helpers: totals & aggregates\n",
    "# Small builders used by the JSON exporter.\n",
    "# (UPDATED: added _agg_city_monthly)\n",
    "\n",
    "# %%\n",
    "def _country_totals(tx: pd.DataFrame):\n",
    "    total_revenue = int(np.rint(tx[\"rev\"].sum()))\n",
    "    customers_cnt = int(tx[\"shopUserId\"].nunique())\n",
    "    total_orders = int(tx[\"orderId\"].nunique())\n",
    "    aov_country = None if total_orders == 0 else int(round(float(total_revenue) / total_orders))\n",
    "    return total_revenue, customers_cnt, total_orders, aov_country\n",
    "\n",
    "def _agg_city(tx: pd.DataFrame):\n",
    "    agg_city = (\n",
    "        tx.groupby(\"city\", dropna=False, sort=False)\n",
    "        .agg(total_revenue_sek=(\"rev\", \"sum\"), customers_count=(\"shopUserId\", \"nunique\"))\n",
    "    )\n",
    "    agg_city[\"total_revenue_sek\"] = np.rint(agg_city[\"total_revenue_sek\"]).astype(\"int64\")\n",
    "    agg_city[\"customers_count\"] = agg_city[\"customers_count\"].astype(\"int64\")\n",
    "    city_orders = tx.groupby(\"city\", dropna=False)[\"orderId\"].nunique().rename(\"total_orders\").astype(\"int64\")\n",
    "    return agg_city, city_orders\n",
    "\n",
    "def _agg_customer(tx: pd.DataFrame):\n",
    "    tx_cust = tx[tx[\"shopUserId\"].notna()].copy()\n",
    "    agg_customer = (\n",
    "        tx_cust.groupby([\"city\", \"shopUserId\"], dropna=False, sort=False)\n",
    "        .agg(\n",
    "            total_spent_sek=(\"rev\", \"sum\"),\n",
    "            total_orders=(\"orderId\", \"nunique\"),\n",
    "            first_order=(\"created\", \"min\"),\n",
    "            last_order=(\"created\", \"max\"),\n",
    "            age=(\"age\", mode_or_first),\n",
    "            gender=(\"gender\", mode_or_first),\n",
    "        )\n",
    "    )\n",
    "    agg_customer[\"total_spent_sek\"] = np.rint(agg_customer[\"total_spent_sek\"]).astype(\"int64\")\n",
    "    agg_customer = agg_customer.sort_index(level=[\"city\", \"shopUserId\"])\n",
    "    return agg_customer\n",
    "\n",
    "def _agg_order(tx: pd.DataFrame):\n",
    "    tx_cust = tx[tx[\"shopUserId\"].notna()].copy()\n",
    "    agg_order = (\n",
    "        tx_cust.groupby([\"city\", \"shopUserId\", \"orderId\"], dropna=False, sort=False)\n",
    "        .agg(\n",
    "            order_total_sek=(\"rev\", \"sum\"),\n",
    "            n_items=(\"orderId\", \"size\"),\n",
    "            created=(\"created\", \"min\"),\n",
    "            order_type=(\"type\", mode_or_first),\n",
    "            price=(\"price\", mode_or_first),\n",
    "        )\n",
    "    )\n",
    "    agg_order[\"order_total_sek\"] = np.rint(agg_order[\"order_total_sek\"]).astype(\"int64\")\n",
    "    agg_order = agg_order.sort_index(level=[\"city\", \"shopUserId\", \"orderId\"])\n",
    "    return agg_order\n",
    "\n",
    "def _agg_city_monthly(tx: pd.DataFrame) -> dict[str, dict[str, int]]:\n",
    "    \"\"\"\n",
    "    NEW: sum revenue by city and calendar month (YYYY-MM).\n",
    "    Returns {city -> { 'YYYY-MM': total_revenue_sek_int, ... }, ...}\n",
    "    \"\"\"\n",
    "    txm = tx.copy()\n",
    "    # Ensure naive, month string\n",
    "    ym = txm[\"created\"]\n",
    "    if getattr(ym.dt, \"tz\", None) is not None:\n",
    "        ym = ym.dt.tz_localize(None)\n",
    "    txm[\"year_month\"] = ym.dt.to_period(\"M\").astype(str)\n",
    "\n",
    "    g = (\n",
    "        txm.groupby([\"city\", \"year_month\"], dropna=False)[\"rev\"]\n",
    "        .sum()\n",
    "        .round()\n",
    "        .astype(\"int64\")\n",
    "    )\n",
    "    out: dict[str, dict[str, int]] = {}\n",
    "    for (cty, ym), val in g.items():\n",
    "        ckey = \"Unknown\" if pd.isna(cty) else str(cty)\n",
    "        out.setdefault(ckey, {})[ym] = int(val)\n",
    "    return out\n",
    "\n",
    "def _customers_by_channel(tx: pd.DataFrame) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count distinct customers per 'type' (channel).\n",
    "    Assumes 'type' is always one of: 'telephone', 'web', or 'Email'.\n",
    "    \"\"\"\n",
    "    if \"type\" not in tx.columns:\n",
    "        return {}\n",
    "    tmp = tx[[\"shopUserId\", \"type\"]].dropna().copy()\n",
    "\n",
    "    # No normalization needed; just use the type as channel\n",
    "    tmp[\"channel\"] = tmp[\"type\"]\n",
    "\n",
    "    # distinct customers per channel\n",
    "    g = (\n",
    "        tmp.dropna(subset=[\"channel\", \"shopUserId\"])\n",
    "           .groupby(\"channel\")[\"shopUserId\"]\n",
    "           .nunique()\n",
    "           .astype(int)\n",
    "    )\n",
    "    return {k: int(v) for k, v in g.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc6856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _agg_customer(tx: pd.DataFrame):\n",
    "    tx_cust = tx[tx[\"shopUserId\"].notna()].copy()\n",
    "    agg_customer = (\n",
    "        tx_cust.groupby([\"city\", \"shopUserId\"], dropna=False, sort=False)\n",
    "        .agg(\n",
    "            total_spent_sek=(\"rev\", \"sum\"),\n",
    "            total_orders=(\"orderId\", \"nunique\"),\n",
    "            first_order=(\"created\", \"min\"),\n",
    "            last_order=(\"created\", \"max\"),\n",
    "            age=(\"Age\", mode_or_first),\n",
    "            gender=(\"Gender\", mode_or_first),\n",
    "        )\n",
    "    )\n",
    "    agg_customer[\"total_spent_sek\"] = np.rint(agg_customer[\"total_spent_sek\"]).astype(\"int64\")\n",
    "    return agg_customer.sort_index(level=[\"city\", \"shopUserId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a695f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export helpers: items extraction\n",
    "# Build items_tx from original tx aligned to country tx indices.\n",
    "\n",
    "# %%\n",
    "def _build_items_grouped(tx_country: pd.DataFrame, tx: pd.DataFrame, articles: pd.DataFrame | None = None):\n",
    "    item_cols = [\"sku\",\"groupId\",\"created\",\"quantity\",\"price_sek\",\"name\",\"line_total_sek\",\"type\",\"brand\",\"category\",\"price\"]\n",
    "    present   = [c for c in item_cols if c in tx.columns]\n",
    "\n",
    "    items_tx = pd.DataFrame({\"city\": tx_country[\"city\"], \"shopUserId\": tx_country[\"shopUserId\"], \"orderId\": tx_country[\"orderId\"]})\n",
    "    for c in present:\n",
    "        if c == \"created\":\n",
    "            col_created = tx_country[\"created\"]\n",
    "            try:\n",
    "                if getattr(col_created.dt, \"tz\", None) is not None:\n",
    "                    col_created = col_created.dt.tz_localize(None)\n",
    "            except Exception:\n",
    "                pass\n",
    "            items_tx[c] = col_created\n",
    "        else:\n",
    "            items_tx[c] = tx.loc[tx_country.index, c] if c in tx.columns else None\n",
    "\n",
    "    # --- NEW: enrich brand/category from articles ---\n",
    "    if articles is not None:\n",
    "        art = articles.copy()\n",
    "\n",
    "        # normalize join keys\n",
    "        for key in (\"sku\", \"groupId\"):\n",
    "            if key in art.columns:\n",
    "                art[key] = art[key].astype(\"string[python]\").str.strip()\n",
    "        if \"sku\" in items_tx.columns:\n",
    "            items_tx[\"sku\"] = items_tx[\"sku\"].astype(\"string[python]\").str.strip()\n",
    "        if \"groupId\" in items_tx.columns:\n",
    "            items_tx[\"groupId\"] = items_tx[\"groupId\"].astype(\"string[python]\").str.strip()\n",
    "\n",
    "        # ensure target columns exist\n",
    "        if \"brand\" not in items_tx.columns:\n",
    "            items_tx[\"brand\"] = pd.NA\n",
    "        if \"category\" not in items_tx.columns:\n",
    "            items_tx[\"category\"] = pd.NA\n",
    "\n",
    "        # maps by SKU (preferred)\n",
    "        if \"sku\" in art.columns:\n",
    "            if \"brand\" in art.columns:\n",
    "                sku_brand = art.dropna(subset=[\"sku\"]).drop_duplicates(\"sku\").set_index(\"sku\")[\"brand\"]\n",
    "                items_tx[\"brand\"] = items_tx[\"brand\"].fillna(items_tx.get(\"sku\").map(sku_brand))\n",
    "            if \"category\" in art.columns:\n",
    "                sku_cat = art.dropna(subset=[\"sku\"]).drop_duplicates(\"sku\").set_index(\"sku\")[\"category\"]\n",
    "                items_tx[\"category\"] = items_tx[\"category\"].fillna(items_tx.get(\"sku\").map(sku_cat))\n",
    "\n",
    "        # fallback maps by groupId\n",
    "        if \"groupId\" in art.columns:\n",
    "            if \"brand\" in art.columns:\n",
    "                gid_brand = art.dropna(subset=[\"groupId\"]).drop_duplicates(\"groupId\").set_index(\"groupId\")[\"brand\"]\n",
    "                items_tx[\"brand\"] = items_tx[\"brand\"].fillna(items_tx.get(\"groupId\").map(gid_brand))\n",
    "            if \"category\" in art.columns:\n",
    "                gid_cat = art.dropna(subset=[\"groupId\"]).drop_duplicates(\"groupId\").set_index(\"groupId\")[\"category\"]\n",
    "                items_tx[\"category\"] = items_tx[\"category\"].fillna(items_tx.get(\"groupId\").map(gid_cat))\n",
    "\n",
    "    items_tx = items_tx[tx_country[\"shopUserId\"].notna()].copy()\n",
    "    items_tx[\"city\"] = items_tx[\"city\"].fillna(\"Unknown\")\n",
    "    return items_tx.groupby([\"city\",\"shopUserId\",\"orderId\"], dropna=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export helpers: JSON node builders\n",
    "# Convert rows to JSON-friendly dicts.\n",
    "\n",
    "# %%\n",
    "def _item_dict(row: pd.Series):\n",
    "    cr = row.get(\"created\")\n",
    "    if isinstance(cr, pd.Timestamp):\n",
    "        cr = cr.isoformat(sep=\" \")\n",
    "    def nz(v): return None if pd.isna(v) else v\n",
    "    def to_int(v): return None if pd.isna(v) else int(v)\n",
    "    def to_float(v): return None if pd.isna(v) else float(v)\n",
    "    return {\n",
    "        \"sku\": nz(row.get(\"sku\")),\n",
    "        \"groupId\": nz(row.get(\"groupId\")),\n",
    "        \"created\": nz(cr),\n",
    "        \"quantity\": to_int(row.get(\"quantity\")),\n",
    "        \"price_sek\": to_int(row.get(\"price_sek\")),\n",
    "        \"name\": nz(row.get(\"name\")),\n",
    "        \"line_total_sek\": to_int(row.get(\"line_total_sek\")),\n",
    "        \"type\": nz(row.get(\"type\")),\n",
    "        \"brand\": nz(row.get(\"brand\")),\n",
    "        \"category\": nz(row.get(\"category\")),\n",
    "        \"price\": to_float(row.get(\"price\")),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6a4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export: main function\n",
    "# Assemble the JSON structure and write to disk.\n",
    "# (UPDATED: inject per-city monthly revenue)\n",
    "\n",
    "# %%\n",
    "def export_country_json(tx_country: pd.DataFrame, tx_full: pd.DataFrame, country_name: str, out_dir=\"/workspace/data/processed\", articles: pd.DataFrame | None = None):\n",
    "    tx_c = tx_country.copy()\n",
    "    tx_c[\"city\"] = tx_c[\"city\"].fillna(\"Unknown\")\n",
    "\n",
    "    total_revenue, customers_cnt, total_orders, aov_country = _country_totals(tx_c)\n",
    "    agg_city, city_orders = _agg_city(tx_c)\n",
    "    agg_customer = _agg_customer(tx_c)\n",
    "    agg_order = _agg_order(tx_c)\n",
    "    city_monthly_map = _agg_city_monthly(tx_c)\n",
    "    customers_by_channel = _customers_by_channel(tx_c)\n",
    "\n",
    "    items_grouped = _build_items_grouped(tx_c, tx_full, articles=articles)\n",
    " \n",
    "\n",
    "    # ---------- build JSON ----------\n",
    "    top_key = country_name.lower()\n",
    "    result = {\n",
    "        top_key: {\n",
    "            \"total_revenue_sek\": int(total_revenue),\n",
    "            \"customers_count\": int(customers_cnt),\n",
    "            \"total_orders\": int(total_orders),\n",
    "            \"avg_order_value_sek\": aov_country,\n",
    "            \"customers_by_channel\": customers_by_channel,     # << NEW\n",
    "            \"cities\": {},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # cities\n",
    "    for cty, row in agg_city.iterrows():\n",
    "        ckey = \"Unknown\" if pd.isna(cty) else str(cty)\n",
    "        orders_c = int(city_orders.get(cty, 0))\n",
    "        rev_c = int(row[\"total_revenue_sek\"])\n",
    "        aov_c = None if orders_c == 0 else int(round(float(rev_c) / orders_c))\n",
    "\n",
    "        result[top_key][\"cities\"][ckey] = {\n",
    "            \"total_revenue_sek\": rev_c,\n",
    "            \"customers_count\": int(row[\"customers_count\"]),\n",
    "            \"total_orders\": orders_c,\n",
    "            \"avg_order_value_sek\": aov_c,\n",
    "            # NEW: monthly breakdown here\n",
    "            \"monthly_revenue_sek\": city_monthly_map.get(ckey, {}),\n",
    "            \"customers\": {},\n",
    "        }\n",
    "\n",
    "    # customers + orders + items (unchanged)\n",
    "    for (cty, uid), row in agg_customer.iterrows():\n",
    "        status = status_from_orders(int(row[\"total_orders\"]))\n",
    "        first_iso = row[\"first_order\"].isoformat(sep=\" \") if pd.notna(row[\"first_order\"]) else None\n",
    "        last_iso = row[\"last_order\"].isoformat(sep=\" \") if pd.notna(row[\"last_order\"]) else None\n",
    "\n",
    "        age_val = None\n",
    "        if \"age\" in row and pd.notna(row[\"age\"]):\n",
    "            try:\n",
    "                age_val = int(row[\"age\"])\n",
    "            except Exception:\n",
    "                age_val = None\n",
    "        gender_val = None if \"gender\" not in row or pd.isna(row[\"gender\"]) else str(row[\"gender\"])\n",
    "\n",
    "        cust_node = {\n",
    "            \"summary\": {\n",
    "                \"total_orders\": int(row[\"total_orders\"]),\n",
    "                \"total_spent_sek\": int(row[\"total_spent_sek\"]),\n",
    "                \"first_order\": first_iso,\n",
    "                \"last_order\": last_iso,\n",
    "                \"status\": status,\n",
    "                \"age\": age_val,\n",
    "                \"gender\": gender_val,\n",
    "            },\n",
    "            \"orders\": {},\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            cust_orders = agg_order.loc[(cty, uid)]\n",
    "            if isinstance(cust_orders, pd.Series):\n",
    "                cust_orders = cust_orders.to_frame().T\n",
    "            for oid, orow in cust_orders.iterrows():\n",
    "                try:\n",
    "                    items_for_order = items_grouped.get_group((cty, uid, oid))\n",
    "                    items = [_item_dict(r) for _, r in items_for_order.iterrows()]\n",
    "                except KeyError:\n",
    "                    items = []\n",
    "                cust_node[\"orders\"][str(oid)] = {\n",
    "                    \"created\": orow[\"created\"].isoformat(sep=\" \") if pd.notna(orow[\"created\"]) else None,\n",
    "                    \"order_total_sek\": int(orow[\"order_total_sek\"]),\n",
    "                    \"n_items\": int(orow[\"n_items\"]),\n",
    "                    \"order_type\": None if pd.isna(orow[\"order_type\"]) else orow[\"order_type\"],\n",
    "                    \"price\": None if pd.isna(orow.get(\"price\")) else float(orow.get(\"price\")),\n",
    "                    \"items\": items,\n",
    "                }\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        ckey = \"Unknown\" if pd.isna(cty) else str(cty)\n",
    "        result[top_key][\"cities\"][ckey][\"customers\"][str(uid)] = cust_node\n",
    "\n",
    "    # write\n",
    "    out_path = Path(out_dir) / f\"{country_name}.json\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb6c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /workspace/data/processed/Sweden.json\n",
      "Saved: /workspace/data/processed/Denmark.json\n",
      "Saved: /workspace/data/processed/Finland.json\n",
      "Saved: /workspace/data/processed/Norway.json\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Example usage\n",
    "# Split once, export four times.\n",
    "\n",
    "# %%\n",
    "countries = split_nordics(tx)\n",
    "export_country_json(countries[\"Sweden\"],  tx, \"Sweden\", articles=articles)\n",
    "export_country_json(countries[\"Denmark\"], tx, \"Denmark\", articles=articles)\n",
    "export_country_json(countries[\"Finland\"], tx, \"Finland\", articles=articles)\n",
    "export_country_json(countries[\"Norway\"],  tx, \"Norway\",  articles=articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51f7ae",
   "metadata": {},
   "source": [
    "## Flatten json for quick math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a699804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Imports & Futures\n",
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de4a5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Paths & Config\n",
    "\n",
    "# %%\n",
    "# ---- configure these paths ----\n",
    "INPUT_DIR  = Path(\"../data/processed\")     # Sweden.json, Denmark.json, Finland.json, Norway.json\n",
    "OUTPUT_DIR = Path(\"../data/parquet_out\")   # will contain the 5 Parquet files\n",
    "\n",
    "COUNTRY_FILES = {\n",
    "    \"Sweden\":  INPUT_DIR / \"Sweden.json\",\n",
    "    \"Denmark\": INPUT_DIR / \"Denmark.json\",\n",
    "    \"Finland\": INPUT_DIR / \"Finland.json\",\n",
    "    \"Norway\":  INPUT_DIR / \"Norway.json\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c9c8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Stable Column Schemas\n",
    "# (UPDATED: added CS_COUNTRY_CHANNEL)\n",
    "\n",
    "# %%\n",
    "CS_COUNTRY = [\"country\",\"total_revenue_sek\",\"customers_count\",\"total_orders\",\"avg_order_value_sek\"]\n",
    "CS_CITY    = [\"country\",\"city\",\"total_revenue_sek\",\"customers_count\",\"total_orders\",\"avg_order_value_sek\"]\n",
    "\n",
    "# Added age & gender here\n",
    "CS_CUST    = [\"country\",\"city\",\"customer_id\",\"total_orders\",\"total_spent_sek\",\n",
    "              \"first_order\",\"last_order\",\"status\",\"age\",\"gender\"]\n",
    "\n",
    "CS_ORDERS  = [\"country\",\"city\",\"customer_id\",\"order_id\",\"created\",\"order_total_sek\",\"n_items\",\"order_type\",\"price\"]\n",
    "CS_ITEMS   = [\"country\",\"city\",\"customer_id\",\"order_id\",\"sku\",\"groupId\",\"created\",\"quantity\",\"price_sek\",\"name\",\"line_total_sek\",\"type\",\"brand\",\"category\",\"price\"]\n",
    "\n",
    "# monthly revenue per city (existing)\n",
    "CS_CITY_MONTHLY = [\"country\",\"city\",\"year_month\",\"total_revenue_sek\"]\n",
    "\n",
    "# NEW: customers by channel per country\n",
    "CS_COUNTRY_CHANNEL = [\"country\",\"channel\",\"customers_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b4054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # I/O Helpers\n",
    "\n",
    "# %%\n",
    "def load_json(path: Path) -> dict:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_parquet(tx: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tx.to_parquet(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5da16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # DataFrame Utilities\n",
    "\n",
    "# %%\n",
    "def _ensure(tx: pd.DataFrame | None, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Ensure columns exist, preserve dtypes, and reorder.\"\"\"\n",
    "    if tx is None or tx.empty:\n",
    "        return pd.DataFrame({c: pd.Series([], dtype=\"object\") for c in cols})[cols]\n",
    "    for c in cols:\n",
    "        if c not in tx.columns:\n",
    "            tx[c] = pd.NA\n",
    "    return tx[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e7244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # JSON Shape Helpers\n",
    "\n",
    "# %%\n",
    "def _unwrap(obj: dict, hint: str) -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Accept {\"denmark\": {...}} or {...}.\n",
    "    Returns (country_name_capitalized, payload_dict).\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict) and len(obj) == 1 and isinstance(next(iter(obj.values())), dict):\n",
    "        k = next(iter(obj.keys()))\n",
    "        return k.capitalize(), next(iter(obj.values()))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Top-level JSON must be an object/dict\")\n",
    "    return hint, obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad3680b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Flatten: Country → Row Buckets\n",
    "# (UPDATED: collect \"country_channels\" from root.customers_by_channel)\n",
    "\n",
    "# %%\n",
    "def flatten_country(obj: dict, country_hint: str) -> dict[str, list[dict]]:\n",
    "    country, root = _unwrap(obj, country_hint)\n",
    "    out = {\n",
    "        \"country_summary\": [{\n",
    "            \"country\": country,\n",
    "            \"total_revenue_sek\": root.get(\"total_revenue_sek\"),\n",
    "            \"customers_count\": root.get(\"customers_count\"),\n",
    "            \"total_orders\": root.get(\"total_orders\"),\n",
    "            \"avg_order_value_sek\": root.get(\"avg_order_value_sek\"),\n",
    "        }],\n",
    "        \"country_channels\": [],   # NEW\n",
    "        \"city_summary\": [],\n",
    "        \"city_monthly\": [],\n",
    "        \"customer_summary\": [],\n",
    "        \"orders\": [],\n",
    "        \"order_items\": [],\n",
    "    }\n",
    "\n",
    "    # NEW: expand customers_by_channel dict into rows\n",
    "    for ch, cnt in (root.get(\"customers_by_channel\") or {}).items():\n",
    "        out[\"country_channels\"].append({\n",
    "            \"country\": country,\n",
    "            \"channel\": ch,\n",
    "            \"customers_count\": cnt,\n",
    "        })\n",
    "\n",
    "\n",
    "    for city, cnode in (root.get(\"cities\") or {}).items():\n",
    "        out[\"city_summary\"].append({\n",
    "            \"country\": country, \"city\": city,\n",
    "            \"total_revenue_sek\": cnode.get(\"total_revenue_sek\"),\n",
    "            \"customers_count\": cnode.get(\"customers_count\"),\n",
    "            \"total_orders\": cnode.get(\"total_orders\"),\n",
    "            \"avg_order_value_sek\": cnode.get(\"avg_order_value_sek\"),\n",
    "        })\n",
    "\n",
    "        # NEW: monthly map { 'YYYY-MM': revenue }\n",
    "        for ym, rev in (cnode.get(\"monthly_revenue_sek\") or {}).items():\n",
    "            out[\"city_monthly\"].append({\n",
    "                \"country\": country,\n",
    "                \"city\": city,\n",
    "                \"year_month\": ym,\n",
    "                \"total_revenue_sek\": rev,\n",
    "            })\n",
    "\n",
    "        for cust_id, cst in (cnode.get(\"customers\") or {}).items():\n",
    "            summ = cst.get(\"summary\") or {}\n",
    "            out[\"customer_summary\"].append({\n",
    "                \"country\": country,\n",
    "                \"city\": city,\n",
    "                \"customer_id\": cust_id,\n",
    "                \"total_orders\": summ.get(\"total_orders\"),\n",
    "                \"total_spent_sek\": summ.get(\"total_spent_sek\"),\n",
    "                \"first_order\": summ.get(\"first_order\"),\n",
    "                \"last_order\": summ.get(\"last_order\"),\n",
    "                \"status\": summ.get(\"status\"),\n",
    "                \"age\": summ.get(\"age\"),\n",
    "                \"gender\": summ.get(\"gender\"),\n",
    "            })\n",
    "            for order_id, ordn in (cst.get(\"orders\") or {}).items():\n",
    "                out[\"orders\"].append({\n",
    "                    \"country\": country, \"city\": city, \"customer_id\": cust_id, \"order_id\": order_id,\n",
    "                    \"created\": ordn.get(\"created\"),\n",
    "                    \"order_total_sek\": ordn.get(\"order_total_sek\"),\n",
    "                    \"n_items\": ordn.get(\"n_items\"),\n",
    "                    \"order_type\": ordn.get(\"order_type\"),\n",
    "                    \"price\": ordn.get(\"price\"),\n",
    "                })\n",
    "                for it in (ordn.get(\"items\") or []):\n",
    "                    out[\"order_items\"].append({\n",
    "                        \"country\": country, \"city\": city, \"customer_id\": cust_id, \"order_id\": order_id,\n",
    "                        \"sku\": it.get(\"sku\"), \"groupId\": it.get(\"groupId\"), \"created\": it.get(\"created\"),\n",
    "                        \"quantity\": it.get(\"quantity\"), \"price_sek\": it.get(\"price_sek\"), \"name\": it.get(\"name\"),\n",
    "                        \"line_total_sek\": it.get(\"line_total_sek\"), \"type\": it.get(\"type\"),\n",
    "                        \"brand\": it.get(\"brand\"), \"category\": it.get(\"category\"), \"price\": it.get(\"price\"),\n",
    "                    })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bc676bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Aggregation Orchestrator\n",
    "# (UPDATED: include \"city_monthly\" bucket)\n",
    "\n",
    "# %%\n",
    "def collect_buckets(country_files: dict[str, Path]) -> dict[str, list[dict]]:\n",
    "    buckets = {k: [] for k in [\n",
    "        \"country_summary\",\"country_channels\",\"city_summary\",\"city_monthly\",\"customer_summary\",\"orders\",\"order_items\"\n",
    "    ]}\n",
    "    for name, path in country_files.items():\n",
    "        if not path.exists():\n",
    "            print(f\"[warn] missing: {path}\")\n",
    "            continue\n",
    "        rows = flatten_country(load_json(path), name)\n",
    "        for k, v in rows.items():\n",
    "            buckets[k].extend(v)\n",
    "        print(f\"[ok] parsed {name}\")\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac9c7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Materialize DataFrames (Fixed Schemas)\n",
    "# (UPDATED: return tx_city_monthly)\n",
    "\n",
    "# %%\n",
    "def to_dataframes(buckets: dict[str, list[dict]]) -> dict[str, pd.DataFrame]:\n",
    "    tx_country = _ensure(pd.DataFrame(buckets[\"country_summary\"]),   CS_COUNTRY)\n",
    "    tx_cc      = _ensure(pd.DataFrame(buckets[\"country_channels\"]),  CS_COUNTRY_CHANNEL)  # NEW\n",
    "    tx_city    = _ensure(pd.DataFrame(buckets[\"city_summary\"]),      CS_CITY)\n",
    "    tx_city_m  = _ensure(pd.DataFrame(buckets[\"city_monthly\"]),      CS_CITY_MONTHLY)\n",
    "    tx_cust    = _ensure(pd.DataFrame(buckets[\"customer_summary\"]),  CS_CUST)\n",
    "    tx_orders  = _ensure(pd.DataFrame(buckets[\"orders\"]),            CS_ORDERS)\n",
    "    tx_items   = _ensure(pd.DataFrame(buckets[\"order_items\"]),       CS_ITEMS)\n",
    "    return {\n",
    "        \"country_summary\": tx_country,\n",
    "        \"country_channels\": tx_cc,         # NEW\n",
    "        \"city_summary\": tx_city,\n",
    "        \"city_monthly\": tx_city_m,\n",
    "        \"customer_summary\": tx_cust,\n",
    "        \"orders\": tx_orders,\n",
    "        \"order_items\": tx_items,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7d74eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Persist to Parquet\n",
    "\n",
    "# %%\n",
    "def write_all_parquet(txs: dict[str, pd.DataFrame], out_dir: Path) -> None:\n",
    "    save_parquet(txs[\"country_summary\"],   out_dir / \"country_summary.parquet\")\n",
    "    save_parquet(txs[\"country_channels\"],  out_dir / \"country_customers_by_channel.parquet\")  # NEW\n",
    "    save_parquet(txs[\"city_summary\"],      out_dir / \"city_summary.parquet\")\n",
    "    save_parquet(txs[\"city_monthly\"],      out_dir / \"city_monthly_revenue.parquet\")\n",
    "    save_parquet(txs[\"customer_summary\"],  out_dir / \"customer_summary.parquet\")\n",
    "    save_parquet(txs[\"orders\"],            out_dir / \"orders.parquet\")\n",
    "    save_parquet(txs[\"order_items\"],       out_dir / \"order_items.parquet\")\n",
    "    print(f\"[done] wrote Parquet files to {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5932b3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] parsed Sweden\n",
      "[ok] parsed Denmark\n",
      "[ok] parsed Finland\n",
      "[ok] parsed Norway\n",
      "[done] wrote Parquet files to /workspace/data/parquet_out\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Main\n",
    "\n",
    "# %%\n",
    "def main():\n",
    "    buckets = collect_buckets(COUNTRY_FILES)\n",
    "    txs = to_dataframes(buckets)\n",
    "    write_all_parquet(txs, OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b327b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>sku</th>\n",
       "      <th>groupId</th>\n",
       "      <th>created</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_sek</th>\n",
       "      <th>name</th>\n",
       "      <th>line_total_sek</th>\n",
       "      <th>type</th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60194</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Mjölby</td>\n",
       "      <td>245562</td>\n",
       "      <td>256993</td>\n",
       "      <td>352787</td>\n",
       "      <td>352787</td>\n",
       "      <td>2024-08-15 10:01:34</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>Tvättpåsar 4 delar</td>\n",
       "      <td>69</td>\n",
       "      <td>web</td>\n",
       "      <td>Ateljé Margaretha</td>\n",
       "      <td>Tvätt &amp; skötsel,Hushåll övrigt,Vardagshjälpmedel</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44193</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Kiruna</td>\n",
       "      <td>398285</td>\n",
       "      <td>320448</td>\n",
       "      <td>261953-3641</td>\n",
       "      <td>261953</td>\n",
       "      <td>2024-09-25 15:10:44</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>Damstrumpor 5-pack</td>\n",
       "      <td>98</td>\n",
       "      <td>telephone</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Strumpor,Underkläder</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55527</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Varberg</td>\n",
       "      <td>866714</td>\n",
       "      <td>821949</td>\n",
       "      <td>270599-4042</td>\n",
       "      <td>270599</td>\n",
       "      <td>2025-09-03 22:14:22</td>\n",
       "      <td>1</td>\n",
       "      <td>239</td>\n",
       "      <td>V-ringat nattl. blå blom</td>\n",
       "      <td>239</td>\n",
       "      <td>web</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Nattlinnen,Sovkläder</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150137</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>294289</td>\n",
       "      <td>203155</td>\n",
       "      <td>260365-0040</td>\n",
       "      <td>216401</td>\n",
       "      <td>2024-07-03 11:40:32</td>\n",
       "      <td>1</td>\n",
       "      <td>385</td>\n",
       "      <td>Alushousut Adamo Basic</td>\n",
       "      <td>385</td>\n",
       "      <td>web</td>\n",
       "      <td>Swegmark</td>\n",
       "      <td>Trosor,Underkläder,Gördlar</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53180</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Alingsås</td>\n",
       "      <td>284860</td>\n",
       "      <td>192613</td>\n",
       "      <td>263988-3839</td>\n",
       "      <td>263988</td>\n",
       "      <td>2024-06-24 17:06:11</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>Stödknästrumpor i nylon med spets</td>\n",
       "      <td>632</td>\n",
       "      <td>web</td>\n",
       "      <td>Funq Wear</td>\n",
       "      <td>Stödstrumpor,Underkläder</td>\n",
       "      <td>158.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163550</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Lahti</td>\n",
       "      <td>250044</td>\n",
       "      <td>846293</td>\n",
       "      <td>241297-0054</td>\n",
       "      <td>241562</td>\n",
       "      <td>2025-09-19 10:02:21</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>Veluurihousut</td>\n",
       "      <td>573</td>\n",
       "      <td>web</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Byxor,Mjukisbyxor,Nederdelar,Mysplagg</td>\n",
       "      <td>51.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41967</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Brandbergen</td>\n",
       "      <td>841064</td>\n",
       "      <td>387266</td>\n",
       "      <td>200312</td>\n",
       "      <td>200312</td>\n",
       "      <td>2024-11-01 14:11:49</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>Mönsterstickad mössa</td>\n",
       "      <td>98</td>\n",
       "      <td>letter</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Accessoarer,Kepsar &amp; mössor</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209014</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Drammen</td>\n",
       "      <td>200475</td>\n",
       "      <td>767171</td>\n",
       "      <td>292789</td>\n",
       "      <td>292789</td>\n",
       "      <td>2025-07-18 10:35:02</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>Tubeklemme for metalltuber 2-pk</td>\n",
       "      <td>75</td>\n",
       "      <td>web</td>\n",
       "      <td>Good Living</td>\n",
       "      <td>Kökshjälpmedel,Vardagshjälpmedel</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208570</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Fredrikstad</td>\n",
       "      <td>406281</td>\n",
       "      <td>329469</td>\n",
       "      <td>266650-C095</td>\n",
       "      <td>266643</td>\n",
       "      <td>2024-09-30 18:18:12</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>Bh uten bøyle Betty</td>\n",
       "      <td>311</td>\n",
       "      <td>web</td>\n",
       "      <td>Trofé</td>\n",
       "      <td>Bh utan bygel,Bh,Underkläder</td>\n",
       "      <td>329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209998</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Drammen</td>\n",
       "      <td>839616</td>\n",
       "      <td>348295</td>\n",
       "      <td>290211</td>\n",
       "      <td>290211</td>\n",
       "      <td>2024-10-11 11:17:36</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>Urinflaske unisex</td>\n",
       "      <td>131</td>\n",
       "      <td>telephone</td>\n",
       "      <td>Good Living</td>\n",
       "      <td>Inkontinens,Badrum/WC</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country         city customer_id order_id          sku groupId  \\\n",
       "60194    Sweden       Mjölby      245562   256993       352787  352787   \n",
       "44193    Sweden       Kiruna      398285   320448  261953-3641  261953   \n",
       "55527    Sweden      Varberg      866714   821949  270599-4042  270599   \n",
       "150137  Finland     Helsinki      294289   203155  260365-0040  216401   \n",
       "53180    Sweden     Alingsås      284860   192613  263988-3839  263988   \n",
       "163550  Finland        Lahti      250044   846293  241297-0054  241562   \n",
       "41967    Sweden  Brandbergen      841064   387266       200312  200312   \n",
       "209014   Norway      Drammen      200475   767171       292789  292789   \n",
       "208570   Norway  Fredrikstad      406281   329469  266650-C095  266643   \n",
       "209998   Norway      Drammen      839616   348295       290211  290211   \n",
       "\n",
       "                    created  quantity  price_sek  \\\n",
       "60194   2024-08-15 10:01:34         1         69   \n",
       "44193   2024-09-25 15:10:44         1         98   \n",
       "55527   2025-09-03 22:14:22         1        239   \n",
       "150137  2024-07-03 11:40:32         1        385   \n",
       "53180   2024-06-24 17:06:11         4        158   \n",
       "163550  2025-09-19 10:02:21         1        573   \n",
       "41967   2024-11-01 14:11:49         1         98   \n",
       "209014  2025-07-18 10:35:02         1         75   \n",
       "208570  2024-09-30 18:18:12         1        311   \n",
       "209998  2024-10-11 11:17:36         1        131   \n",
       "\n",
       "                                     name  line_total_sek       type  \\\n",
       "60194                  Tvättpåsar 4 delar              69        web   \n",
       "44193                  Damstrumpor 5-pack              98  telephone   \n",
       "55527            V-ringat nattl. blå blom             239        web   \n",
       "150137             Alushousut Adamo Basic             385        web   \n",
       "53180   Stödknästrumpor i nylon med spets             632        web   \n",
       "163550                      Veluurihousut             573        web   \n",
       "41967                Mönsterstickad mössa              98     letter   \n",
       "209014    Tubeklemme for metalltuber 2-pk              75        web   \n",
       "208570                Bh uten bøyle Betty             311        web   \n",
       "209998                  Urinflaske unisex             131  telephone   \n",
       "\n",
       "                    brand                                          category  \\\n",
       "60194   Ateljé Margaretha  Tvätt & skötsel,Hushåll övrigt,Vardagshjälpmedel   \n",
       "44193             unknown                              Strumpor,Underkläder   \n",
       "55527             unknown                              Nattlinnen,Sovkläder   \n",
       "150137           Swegmark                        Trosor,Underkläder,Gördlar   \n",
       "53180           Funq Wear                          Stödstrumpor,Underkläder   \n",
       "163550             Åshild             Byxor,Mjukisbyxor,Nederdelar,Mysplagg   \n",
       "41967              Åshild                       Accessoarer,Kepsar & mössor   \n",
       "209014        Good Living                  Kökshjälpmedel,Vardagshjälpmedel   \n",
       "208570              Trofé                      Bh utan bygel,Bh,Underkläder   \n",
       "209998        Good Living                             Inkontinens,Badrum/WC   \n",
       "\n",
       "        price  \n",
       "60194    69.0  \n",
       "44193    98.0  \n",
       "55527   239.0  \n",
       "150137   34.9  \n",
       "53180   158.4  \n",
       "163550   51.9  \n",
       "41967    98.0  \n",
       "209014   79.0  \n",
       "208570  329.0  \n",
       "209998  139.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"/workspace/data/parquet_out/\")\n",
    "\n",
    "order_items_tx = pd.read_parquet(OUTPUT_DIR / \"order_items.parquet\")\n",
    "order_items_tx.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eed2682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_categories_by_season.parquet\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Top Categories by Season\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _season_from_month(m: int) -> str:\n",
    "    # Winter: Dec–Feb, Spring: Mar–May, Summer: Jun–Aug, Autumn: Sep–Nov\n",
    "    if m in (12, 1, 2):   return \"Winter\"\n",
    "    if m in (3, 4, 5):    return \"Spring\"\n",
    "    if m in (6, 7, 8):    return \"Summer\"\n",
    "    return \"Autumn\"\n",
    "\n",
    "def build_top_categories_by_season(\n",
    "    tx_items: pd.DataFrame,\n",
    "    category_sep: str = \",\",\n",
    "    keep_top_n: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    tx = tx_items.copy()\n",
    "\n",
    "    # --- types & defaults ---\n",
    "    tx[\"created\"]  = pd.to_datetime(tx[\"created\"], errors=\"coerce\")\n",
    "    tx[\"quantity\"] = pd.to_numeric(tx.get(\"quantity\"), errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "\n",
    "    def _norm(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(\"string[python]\")\n",
    "        s = s.where(~s.isna(), \"Unknown\")\n",
    "        return s.str.strip().fillna(\"Unknown\")\n",
    "\n",
    "    tx[\"country\"]  = _norm(tx.get(\"country\"))\n",
    "    tx[\"category\"] = _norm(tx.get(\"category\"))\n",
    "\n",
    "    # --- season label (Winter labeled by December’s year: Dec–Feb) ---\n",
    "    m = tx[\"created\"].dt.month\n",
    "    y = tx[\"created\"].dt.year\n",
    "    season = m.map(_season_from_month).fillna(\"Unknown\")\n",
    "    season_year = y - (m <= 2).astype(int)  # Jan–Feb belong to previous December’s year\n",
    "    tx[\"season_label\"] = (season + \" \" + season_year.astype(str)).astype(\"string[python]\")\n",
    "\n",
    "    # --- explode categories (no sharing → integer counts) ---\n",
    "    cat_lists = (\n",
    "        tx[\"category\"]\n",
    "          .str.split(category_sep)\n",
    "          .apply(lambda parts: list(dict.fromkeys([p.strip() for p in (parts or []) if p and p.strip()])))\n",
    "    )\n",
    "    tx_exp = tx.loc[cat_lists.index, [\"country\",\"season_label\",\"quantity\"]].copy()\n",
    "    tx_exp[\"category\"] = cat_lists\n",
    "    tx_exp = tx_exp.explode(\"category\", ignore_index=True)\n",
    "    tx_exp[\"category\"] = _norm(tx_exp[\"category\"])\n",
    "\n",
    "    # --- aggregate & rank ---\n",
    "    agg = (\n",
    "        tx_exp.groupby([\"country\",\"season_label\",\"category\"], dropna=False)[\"quantity\"]\n",
    "              .sum().reset_index(name=\"count\")\n",
    "    )\n",
    "    agg[\"count\"] = agg[\"count\"].astype(\"int64\")\n",
    "\n",
    "    agg = agg.sort_values([\"country\",\"season_label\",\"count\"], ascending=[True, True, False])\n",
    "    agg[\"rank\"] = (\n",
    "        agg.groupby([\"country\",\"season_label\"])[\"count\"]\n",
    "           .rank(method=\"first\", ascending=False).astype(int)\n",
    "    )\n",
    "\n",
    "    top = (\n",
    "        agg[agg[\"rank\"] <= keep_top_n]\n",
    "        .sort_values([\"country\",\"season_label\",\"rank\"], ascending=[True, True, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # final tidy schema\n",
    "    return top[[\"country\",\"season_label\",\"category\",\"count\",\"rank\"]]\n",
    "\n",
    "# --- usage ---\n",
    "items_path = OUTPUT_DIR / \"order_items.parquet\"\n",
    "top_path   = OUTPUT_DIR / \"top_categories_by_season.parquet\"\n",
    "tx_items = pd.read_parquet(items_path)\n",
    "tx_top = build_top_categories_by_season(tx_items)\n",
    "tx_top.to_parquet(top_path, index=False)\n",
    "print(f\"[done] wrote {top_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29cb560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_groupids_by_season.parquet\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Top Products by Season\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def _season_from_month(m: int) -> str:\n",
    "    if m in (12, 1, 2):   return \"Winter\"\n",
    "    if m in (3, 4, 5):    return \"Spring\"\n",
    "    if m in (6, 7, 8):    return \"Summer\"\n",
    "    return \"Autumn\"\n",
    "\n",
    "def build_top_groupids_by_season(\n",
    "    tx_items: pd.DataFrame,\n",
    "    keep_top_n: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    tx = tx_items.copy()\n",
    "\n",
    "    # --- types & defaults ---\n",
    "    tx[\"created\"]  = pd.to_datetime(tx[\"created\"], errors=\"coerce\")\n",
    "    tx[\"quantity\"] = pd.to_numeric(tx.get(\"quantity\"), errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "\n",
    "    def _norm(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(\"string[python]\")\n",
    "        s = s.where(~s.isna(), \"Unknown\")\n",
    "        return s.str.strip().fillna(\"Unknown\")\n",
    "\n",
    "    tx[\"country\"] = _norm(tx.get(\"country\"))\n",
    "    tx[\"groupId\"] = _norm(tx.get(\"groupId\"))\n",
    "    tx[\"brand\"]   = _norm(tx.get(\"brand\"))\n",
    "\n",
    "    # choose a product-name column if present; normalize\n",
    "    if   \"name\" in tx.columns:         base_name = tx[\"name\"]\n",
    "    elif \"productName\" in tx.columns:  base_name = tx[\"productName\"]\n",
    "    elif \"title\" in tx.columns:        base_name = tx[\"title\"]\n",
    "    elif \"product_name\" in tx.columns: base_name = tx[\"product_name\"]\n",
    "    else:                              base_name = pd.Series(pd.NA, index=tx.index)\n",
    "    tx[\"name\"] = _norm(base_name)\n",
    "\n",
    "    # --- season label (Winter labeled by December’s year: Dec–Feb) ---\n",
    "    m = tx[\"created\"].dt.month\n",
    "    y = tx[\"created\"].dt.year\n",
    "    season = m.map(_season_from_month).fillna(\"Unknown\")\n",
    "    season_year = y - (m <= 2).astype(int)  # Jan–Feb belong to previous December’s year\n",
    "    tx[\"season_label\"] = (season + \" \" + season_year.astype(str)).astype(\"string[python]\")\n",
    "\n",
    "    # --- aggregate counts by groupId ---\n",
    "    gid_agg = (\n",
    "        tx.groupby([\"country\",\"season_label\",\"groupId\"], dropna=False)[\"quantity\"]\n",
    "          .sum()\n",
    "          .reset_index(name=\"count\")\n",
    "    )\n",
    "    gid_agg[\"count\"] = gid_agg[\"count\"].astype(\"int64\")\n",
    "\n",
    "    # representative (name, brand) per (country, season_label, groupId)\n",
    "    rep_meta = (\n",
    "        tx.groupby([\"country\",\"season_label\",\"groupId\",\"name\",\"brand\"], dropna=False)[\"quantity\"]\n",
    "          .sum().reset_index(name=\"qty\")\n",
    "          .sort_values([\"country\",\"season_label\",\"groupId\",\"qty\",\"name\",\"brand\"],\n",
    "                       ascending=[True, True, True, False, True, True])\n",
    "          .drop_duplicates([\"country\",\"season_label\",\"groupId\"])\n",
    "          .rename(columns={\"name\":\"rep_name\",\"brand\":\"rep_brand\"})\n",
    "          [[\"country\",\"season_label\",\"groupId\",\"rep_name\",\"rep_brand\"]]\n",
    "    )\n",
    "    gid_agg = gid_agg.merge(rep_meta, on=[\"country\",\"season_label\",\"groupId\"], how=\"left\")\n",
    "\n",
    "    # --- rank within (country, season) and take top N ---\n",
    "    gid_agg = gid_agg.sort_values([\"country\",\"season_label\",\"count\"], ascending=[True, True, False])\n",
    "    gid_agg[\"rank\"] = (\n",
    "        gid_agg.groupby([\"country\",\"season_label\"])[\"count\"]\n",
    "               .rank(method=\"first\", ascending=False).astype(int)\n",
    "    )\n",
    "    top = (\n",
    "        gid_agg[gid_agg[\"rank\"] <= keep_top_n]\n",
    "        .sort_values([\"country\",\"season_label\",\"rank\"])\n",
    "        .rename(columns={\"groupId\":\"value\", \"rep_name\":\"name\", \"rep_brand\":\"brand\"})\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # final schema (now includes brand)\n",
    "    return top[[\"country\",\"season_label\",\"value\",\"name\",\"brand\",\"count\",\"rank\"]]\n",
    "\n",
    "# --- usage ---\n",
    "items_path = OUTPUT_DIR / \"order_items.parquet\"\n",
    "top_path   = OUTPUT_DIR / \"top_groupids_by_season.parquet\"\n",
    "tx_items = pd.read_parquet(items_path)\n",
    "tx_top = build_top_groupids_by_season(tx_items)\n",
    "tx_top.to_parquet(top_path, index=False)\n",
    "print(f\"[done] wrote {top_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebaebd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Top Repurchased Products by Country\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_top_repurchase_groupids_by_country_unique_days(\n",
    "    tx_items: pd.DataFrame,\n",
    "    unique_days_threshold: int = 1,   # \"more than 2 different dates\"\n",
    "    keep_top_n: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    tx = tx_items.copy()\n",
    "\n",
    "    # --- types & normalization ---\n",
    "    tx[\"created\"] = pd.to_datetime(tx.get(\"created\"), errors=\"coerce\")\n",
    "    tx = tx.dropna(subset=[\"created\"])\n",
    "    tx[\"purchase_date\"] = tx[\"created\"].dt.normalize()  # keeps datetime64[ns]\n",
    "\n",
    "    # quantity: safe default = 1 if column missing\n",
    "    if \"quantity\" in tx.columns:\n",
    "        q = pd.to_numeric(tx[\"quantity\"], errors=\"coerce\")\n",
    "        tx[\"quantity\"] = q.fillna(1).astype(\"int64\")\n",
    "    else:\n",
    "        tx[\"quantity\"] = 1\n",
    "\n",
    "    def _safe_series(colname: str) -> pd.Series:\n",
    "        return tx[colname] if colname in tx.columns else pd.Series(pd.NA, index=tx.index)\n",
    "\n",
    "    def _norm(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(\"string[python]\")\n",
    "        s = s.where(~s.isna(), \"Unknown\")\n",
    "        return s.str.strip().fillna(\"Unknown\")\n",
    "\n",
    "    tx[\"country\"]     = _norm(_safe_series(\"country\"))\n",
    "    tx[\"groupId\"]     = _norm(_safe_series(\"groupId\"))\n",
    "    tx[\"customer_id\"] = _norm(_safe_series(\"customer_id\"))\n",
    "    tx[\"brand\"]       = _norm(_safe_series(\"brand\"))\n",
    "\n",
    "    # best-available product name column\n",
    "    for cand in [\"name\",\"productName\",\"title\",\"product_name\"]:\n",
    "        if cand in tx.columns:\n",
    "            base_name = tx[cand]\n",
    "            break\n",
    "    else:\n",
    "        base_name = pd.Series(pd.NA, index=tx.index)\n",
    "    tx[\"name\"] = _norm(base_name)\n",
    "\n",
    "    # ---------- per customer, per (country, groupId): count UNIQUE purchase dates ----------\n",
    "    per_cust = (\n",
    "        tx.groupby([\"country\",\"groupId\",\"customer_id\"], dropna=False)\n",
    "          .agg(unique_days=(\"purchase_date\", pd.Series.nunique))\n",
    "          .reset_index()\n",
    "    )\n",
    "    eligible = per_cust[per_cust[\"unique_days\"] > unique_days_threshold].copy()\n",
    "\n",
    "    # ---------- # of repurchasing customers per (country, groupId) ----------\n",
    "    rep_counts = (\n",
    "        eligible.groupby([\"country\",\"groupId\"], dropna=False)[\"customer_id\"]\n",
    "                .nunique().reset_index(name=\"repurchasers\")\n",
    "        .astype({\"repurchasers\":\"int64\"})\n",
    "    )\n",
    "\n",
    "    # ---------- representative (name, brand) computed among repurchasers only ----------\n",
    "    rep_keys = eligible[[\"country\",\"groupId\",\"customer_id\"]]\n",
    "    tx_rep = tx.merge(rep_keys, on=[\"country\",\"groupId\",\"customer_id\"], how=\"inner\")\n",
    "\n",
    "    name_weight = (\n",
    "        tx_rep.groupby([\"country\",\"groupId\",\"name\",\"brand\"], dropna=False)[\"quantity\"]\n",
    "              .sum().reset_index(name=\"qty\")\n",
    "    )\n",
    "    # pick (name, brand) with highest qty; tie-break by name, brand for determinism\n",
    "    rep_meta = (\n",
    "        name_weight.sort_values([\"country\",\"groupId\",\"qty\",\"name\",\"brand\"],\n",
    "                                ascending=[True, True, False, True, True])\n",
    "                   .drop_duplicates([\"country\",\"groupId\"])\n",
    "                   .rename(columns={\"name\":\"rep_name\", \"brand\":\"rep_brand\"})\n",
    "                   [[\"country\",\"groupId\",\"rep_name\",\"rep_brand\"]]\n",
    "    )\n",
    "\n",
    "    out = rep_counts.merge(rep_meta, on=[\"country\",\"groupId\"], how=\"left\")\n",
    "\n",
    "    # ---------- rank within country & top-N ----------\n",
    "    out = out.sort_values([\"country\",\"repurchasers\"], ascending=[True, False])\n",
    "    out[\"rank\"] = out.groupby(\"country\")[\"repurchasers\"].rank(method=\"first\", ascending=False).astype(int)\n",
    "\n",
    "    top = (out[out[\"rank\"] <= keep_top_n]\n",
    "           .sort_values([\"country\",\"rank\"])\n",
    "           .rename(columns={\"groupId\":\"value\", \"rep_name\":\"name\", \"rep_brand\":\"brand\"})\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    return top[[\"country\",\"value\",\"name\",\"brand\",\"repurchasers\",\"rank\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3d1a9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_repurchase_groupids_by_country.parquet\n",
      " value                                                         name         brand  repurchasers  rank\n",
      "260695                                              Seamless BH-top        Louise            75     1\n",
      "260646                                            Dametrusser 3-pak        Åshild            55     2\n",
      "241562                                                 Velourbukser        Åshild            54     3\n",
      "240187                                                  Fritidsbuks        Åshild            53     4\n",
      "260513                                                BH uden bøjle     Glamorise            52     5\n",
      "261637                                              Ankelsokker VID Locköstrumpan            47     6\n",
      "210338                                                T-shirt 2-pak        Åshild            42     7\n",
      "265843 Bomulds-BH uden bøjle med Magic Lift-function Cotton Support     Glamorise            41     8\n",
      "263988                                             Støtteknæstrømpe     Funq Wear            37     9\n",
      "263855                                                BH uden bøjle     Glamorise            33    10\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "items_path = Path(\"../data/parquet_out/order_items.parquet\")\n",
    "tx_items = pd.read_parquet(items_path)\n",
    "\n",
    "tx_top_repurchase_country = build_top_repurchase_groupids_by_country_unique_days(\n",
    "    tx_items,\n",
    ")\n",
    "\n",
    "\n",
    "out_path = Path(\"../data/parquet_out/top_repurchase_groupids_by_country.parquet\")\n",
    "tx_top_repurchase_country.to_parquet(out_path, index=False)\n",
    "print(f\"[done] wrote {out_path.resolve()}\")\n",
    "\n",
    "# quick peek for Denmark\n",
    "print(\n",
    "    tx_top_repurchase_country.query(\"country == 'Denmark'\")\n",
    "                             .sort_values(\"rank\")[[\"value\",\"name\", \"brand\", \"repurchasers\",\"rank\"]]\n",
    "                             .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "210c1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Top Brands by Country\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_top_brands_by_country(tx: pd.DataFrame, keep_top_n: int = 10) -> pd.DataFrame:\n",
    "    tx = tx.copy()\n",
    "    tx[\"quantity\"] = pd.to_numeric(tx[\"quantity\"], errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "    tx[\"country\"]  = tx[\"country\"].astype(\"string\")\n",
    "    tx[\"brand\"]    = tx[\"brand\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # exclude Unknown/unknown/na/empty and NA values\n",
    "    mask = tx[\"brand\"].notna() & ~tx[\"brand\"].str.lower().isin({\"unknown\", \"na\", \"\"})\n",
    "    tx = tx[mask]\n",
    "\n",
    "    agg = (tx.groupby([\"country\",\"brand\"], dropna=False)[\"quantity\"]\n",
    "             .sum().reset_index(name=\"count\").astype({\"count\":\"int64\"}))\n",
    "    agg = agg.sort_values([\"country\",\"count\",\"brand\"], ascending=[True, False, True])\n",
    "    agg[\"rank\"] = agg.groupby(\"country\")[\"count\"].rank(method=\"first\", ascending=False).astype(int)\n",
    "\n",
    "    return (agg[agg[\"rank\"] <= keep_top_n]\n",
    "              .sort_values([\"country\",\"rank\"])\n",
    "              .reset_index(drop=True)[[\"country\",\"brand\",\"count\",\"rank\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11a343f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_brands_by_country.parquet\n",
      "        brand  count  rank\n",
      "       Åshild  15409     1\n",
      "       Louise   6434     2\n",
      "    Glamorise   2847     3\n",
      "  Good Living   2485     4\n",
      "    Miss Mary   2300     5\n",
      "       Sloggi   1146     6\n",
      "Locköstrumpan   1054     7\n",
      "        Trofé    886     8\n",
      "     Swegmark    786     9\n",
      "    Funq Wear    753    10\n"
     ]
    }
   ],
   "source": [
    "tx_top_brands = build_top_brands_by_country(tx_items, keep_top_n=10)\n",
    "\n",
    "out_path = Path(\"../data/parquet_out/top_brands_by_country.parquet\")\n",
    "tx_top_brands.to_parquet(out_path, index=False)\n",
    "print(f\"[done] wrote {out_path.resolve()}\")\n",
    "\n",
    "# quick peek for Denmark\n",
    "print(\n",
    "    tx_top_brands.query(\"country == 'Denmark'\")\n",
    "                    .sort_values(\"rank\")[[\"brand\",\"count\",\"rank\"]]\n",
    "                    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366011",
   "metadata": {},
   "source": [
    "Each customer is counted once, in a mutually exclusive bucket based on the time from their first purchase to their first return on a different date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67ced722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Return Buckets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def bucket_return_days(d: int) -> str | pd.NA:\n",
    "    if pd.isna(d) or d <= 0:          return pd.NA          # no return or same-day only\n",
    "    if 1 <= d <= 7:                   return \"week 1\"\n",
    "    if 8 <= d <= 14:                  return \"week 2\"\n",
    "    if 15 <= d <= 21:                 return \"week 3\"\n",
    "    if 22 <= d <= 30:                 return \"1 month\"\n",
    "    # months as 30-day blocks up to 12 months\n",
    "    for m in range(2, 13):            # 2..12 months\n",
    "        lo, hi = 30*(m-1)+1, 30*m     # e.g., 31-60, 61-90, ...\n",
    "        if lo <= d <= hi:             return f\"{m} months\"\n",
    "    if d > 365:                       return \"> 1 year\"\n",
    "    return pd.NA\n",
    "\n",
    "def count_return_buckets(tx_items: pd.DataFrame) -> pd.DataFrame:\n",
    "    tx = tx_items.copy()\n",
    "    tx[\"created\"] = pd.to_datetime(tx[\"created\"], errors=\"coerce\")\n",
    "    tx = tx.dropna(subset=[\"created\", \"customer_id\"])\n",
    "    tx[\"purchase_date\"] = tx[\"created\"].dt.date\n",
    "\n",
    "    # unique purchase dates per customer\n",
    "    uniq = tx[[\"customer_id\",\"purchase_date\"]].drop_duplicates()\n",
    "\n",
    "    # first purchase per customer\n",
    "    first_date = uniq.groupby(\"customer_id\")[\"purchase_date\"].min().rename(\"first_date\")\n",
    "\n",
    "    # earliest purchase strictly AFTER first_date → first return date\n",
    "    tmp = uniq.merge(first_date, on=\"customer_id\")\n",
    "    tmp = tmp[tmp[\"purchase_date\"] > tmp[\"first_date\"]]\n",
    "    first_return = tmp.groupby(\"customer_id\")[\"purchase_date\"].min().rename(\"first_return_date\")\n",
    "\n",
    "    # join and compute delta days\n",
    "    timeline = first_date.to_frame().merge(first_return, left_index=True, right_index=True, how=\"left\")\n",
    "    timeline[\"days_to_return\"] = (pd.to_datetime(timeline[\"first_return_date\"]) - \n",
    "                                  pd.to_datetime(timeline[\"first_date\"])).dt.days\n",
    "\n",
    "    # bucketize (mutually exclusive)\n",
    "    timeline[\"bucket\"] = timeline[\"days_to_return\"].apply(bucket_return_days)\n",
    "\n",
    "    # keep only customers who returned (bucket not NA)\n",
    "    ret = timeline.dropna(subset=[\"bucket\"])\n",
    "\n",
    "    # counts by bucket\n",
    "    counts = (ret.groupby(\"bucket\").size().reset_index(name=\"customers\")\n",
    "                .sort_values(\"customers\", ascending=False)\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "    # optional: enforce a readable bucket order\n",
    "    order = ([\"week 1\",\"week 2\",\"week 3\",\"1 month\"] +\n",
    "             [f\"{m} months\" for m in range(2,13)] + [\"> 1 year\"])\n",
    "    counts[\"order\"] = counts[\"bucket\"].map({b:i for i,b in enumerate(order)})\n",
    "    counts = counts.sort_values(\"order\").drop(columns=\"order\").reset_index(drop=True)\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c71fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bucket  customers\n",
      "0      week 1       1284\n",
      "1      week 2       1714\n",
      "2      week 3       1219\n",
      "3     1 month       1262\n",
      "4    2 months       3047\n",
      "5    3 months       2444\n",
      "6    4 months       2188\n",
      "7    5 months       1769\n",
      "8    6 months       1994\n",
      "9    7 months       1640\n",
      "10   8 months       1285\n",
      "11   9 months        890\n",
      "12  10 months        722\n",
      "13  11 months        606\n",
      "14  12 months        464\n",
      "15   > 1 year        586\n"
     ]
    }
   ],
   "source": [
    "# tx_items = pd.read_parquet(\"data/order_items.parquet\")\n",
    "tx_buckets = count_return_buckets(tx_items)\n",
    "print(tx_buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66661cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_buckets.to_parquet(\"../data/parquet_out/return_buckets_overall.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a56d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (container)",
   "language": "python",
   "name": "py310-vscode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
