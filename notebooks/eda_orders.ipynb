{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Low-CPU mode\n",
    "#import os\n",
    "#for k in (\"ARROW_NUM_THREADS\", \"OMP_NUM_THREADS\", \"MKL_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"NUMEXPR_MAX_THREADS\"):\n",
    "#    os.environ.setdefault(k, \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fb56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tx = pd.read_parquet(\"../data/processed/transactions_clean.parquet\")\n",
    "customers = pd.read_parquet(\"../data/processed/customers_clean.parquet\")\n",
    "articles  = pd.read_parquet(\"../data/processed/articles_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b670705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- customers ---\n",
      "Age column: 'Age' dtype=float64\n",
      " non-null=36592, unique=87\n",
      " min=17.0, max=103.0\n",
      " sample uniques: [17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0]\n",
      "Gender column: 'Gender' dtype=object\n",
      " unique (non-null): ['female', 'male']\n",
      "\n",
      " value counts:\n",
      " Gender\n",
      "female    34836\n",
      "male       1756\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# === Debug: unique Age / Gender in customers only ===\n",
    "import pandas as pd\n",
    "\n",
    "def _find_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "age_col = _find_col(customers, [\"Age\",\"age\",\"customer_age\"])\n",
    "gen_col = _find_col(customers, [\"Gender\",\"gender\",\"customer_gender\"])\n",
    "\n",
    "print(\"--- customers ---\")\n",
    "if age_col:\n",
    "    ages = pd.to_numeric(customers[age_col], errors=\"coerce\")\n",
    "    print(f\"Age column: '{age_col}' dtype={customers[age_col].dtype}\")\n",
    "    print(f\" non-null={ages.notna().sum()}, unique={ages.dropna().nunique()}\")\n",
    "    if ages.notna().any():\n",
    "        print(f\" min={float(ages.min())}, max={float(ages.max())}\")\n",
    "        print(\" sample uniques:\", sorted(ages.dropna().unique().tolist())[:20])\n",
    "else:\n",
    "    print(\"Age column: NOT FOUND\")\n",
    "\n",
    "if gen_col:\n",
    "    g = customers[gen_col].astype(\"string[python]\").str.strip().str.lower()\n",
    "    print(f\"Gender column: '{gen_col}' dtype={customers[gen_col].dtype}\")\n",
    "    print(\" unique (non-null):\", sorted(g.dropna().unique().tolist()))\n",
    "    print(\"\\n value counts:\\n\", g.value_counts(dropna=True))\n",
    "else:\n",
    "    print(\"Gender column: NOT FOUND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11111154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shopUserId                  object\n",
       "invoiceFirstName    string[python]\n",
       "invoiceLastName     string[python]\n",
       "invoiceSSN                  object\n",
       "invoiceZip          string[python]\n",
       "invoiceCity                 object\n",
       "invoiceCountryId            object\n",
       "invoiceEmail                object\n",
       "_row                         int64\n",
       "Gender                      object\n",
       "Age                        float64\n",
       "Country                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e76633",
   "metadata": {},
   "source": [
    "## Build json for mental mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66547eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Setup & Constants\n",
    "# Lightweight imports and shared constants.\n",
    "\n",
    "# %%\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "NORDICS = [\"Sweden\", \"Denmark\", \"Finland\", \"Norway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbeea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Generic helpers\n",
    "# Small utilities reused across the pipeline.\n",
    "\n",
    "# %%\n",
    "def status_from_orders(n: int) -> str:\n",
    "    return \"new\" if n <= 1 else (\"returning\" if n <= 3 else \"loyal\")\n",
    "\n",
    "def mode_or_first(s: pd.Series):\n",
    "    m = s.mode()\n",
    "    if not m.empty:\n",
    "        return m.iat[0]\n",
    "    s = s.dropna()\n",
    "    return s.iat[0] if not s.empty else None\n",
    "\n",
    "def _pick(df: pd.DataFrame, names, default=None) -> pd.Series:\n",
    "    for n in names:\n",
    "        if n in df.columns:\n",
    "            return df[n]\n",
    "    return pd.Series([default] * len(df), index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f454321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Normalization helpers\n",
    "# Standardize ids and small categorical fields.\n",
    "\n",
    "# %%\n",
    "def _norm_id(s: pd.Series) -> pd.Series:\n",
    "    # to string, strip, drop trailing \".0\" if present (common after Parquet/float cast)\n",
    "    s = s.astype(\"string[python]\").str.strip()\n",
    "    return s.str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "\n",
    "def _norm_gender(s: pd.Series):\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = s.astype(\"string[python]\").str.strip().str.lower()\n",
    "    return s.replace({\"f\": \"female\", \"m\": \"male\", \"kvinnan\": \"female\", \"man\": \"male\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40c4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Tx base-frame builder\n",
    "# Make a clean base transaction frame with unified schema.\n",
    "\n",
    "# %%\n",
    "def _build_base_tx(tx: pd.DataFrame) -> pd.DataFrame:\n",
    "    country = _pick(tx, [\"currency_country\", \"Country\"], \"Unknown\").astype(\"string[python]\").str.strip()\n",
    "    country = country.replace({\"\": pd.NA}).fillna(\"Unknown\")\n",
    "    city = _pick(tx, [\"invoiceCity\", \"city\"], \"Unknown\").astype(object).fillna(\"Unknown\")\n",
    "    shop = _norm_id(tx[\"shopUserId\"])\n",
    "    order_id = tx[\"orderId\"].astype(\"string[python]\").str.strip()\n",
    "\n",
    "    created_raw = tx[\"created\"]\n",
    "    created = created_raw if np.issubdtype(created_raw.dtype, np.datetime64) else pd.to_datetime(created_raw, errors=\"coerce\")\n",
    "    if isinstance(created, pd.Series) and np.issubdtype(created.dtype, np.datetime64):\n",
    "        try:\n",
    "            if getattr(created.dt, \"tz\", None) is not None:\n",
    "                created = created.dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    rev = pd.to_numeric(tx.get(\"line_total_sek\"), errors=\"coerce\").fillna(0)\n",
    "    typ = tx[\"type\"] if \"type\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "    price = tx[\"price\"] if \"price\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"shopUserId\": shop,\n",
    "            \"orderId\": order_id,\n",
    "            \"rev\": rev,\n",
    "            \"created\": created,\n",
    "            \"type\": typ,\n",
    "            \"price\": price,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Tx base-frame builder\n",
    "# Make a clean base transaction frame with unified schema.\n",
    "\n",
    "# %%\n",
    "def _build_base_tx(tx: pd.DataFrame) -> pd.DataFrame:\n",
    "    country = _pick(tx, [\"currency_country\", \"Country\"], \"Unknown\").astype(\"string[python]\").str.strip()\n",
    "    country = country.replace({\"\": pd.NA}).fillna(\"Unknown\")\n",
    "    city = _pick(tx, [\"invoiceCity\", \"city\"], \"Unknown\").astype(object).fillna(\"Unknown\")\n",
    "    shop = _norm_id(tx[\"shopUserId\"])\n",
    "    order_id = tx[\"orderId\"].astype(\"string[python]\").str.strip()\n",
    "\n",
    "    created_raw = tx[\"created\"]\n",
    "    created = created_raw if np.issubdtype(created_raw.dtype, np.datetime64) else pd.to_datetime(created_raw, errors=\"coerce\")\n",
    "    if isinstance(created, pd.Series) and np.issubdtype(created.dtype, np.datetime64):\n",
    "        try:\n",
    "            if getattr(created.dt, \"tz\", None) is not None:\n",
    "                created = created.dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    rev = pd.to_numeric(tx.get(\"line_total_sek\"), errors=\"coerce\").fillna(0)\n",
    "    typ = tx[\"type\"] if \"type\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "    price = tx[\"price\"] if \"price\" in tx.columns else pd.Series([None] * len(tx), index=tx.index)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"shopUserId\": shop,\n",
    "            \"orderId\": order_id,\n",
    "            \"rev\": rev,\n",
    "            \"created\": created,\n",
    "            \"type\": typ,\n",
    "            \"price\": price,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cadbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Customer normalization & collapse\n",
    "# Normalize ids, pick Age/Gender variants, and collapse duplicates.\n",
    "\n",
    "# %%\n",
    "def _prep_customers(customers: pd.DataFrame) -> pd.DataFrame:\n",
    "    c = customers.copy()\n",
    "    c[\"shopUserId_norm\"] = _norm_id(c[\"shopUserId\"])\n",
    "    c[\"Age\"] = pd.to_numeric(_pick(c, [\"Age\", \"age\", \"customer_age\"]), errors=\"coerce\").astype(\"Float64\")\n",
    "    c[\"Gender\"] = _pick(c, [\"Gender\", \"gender\", \"customer_gender\"])\n",
    "    c_agg = (\n",
    "        c.groupby(\"shopUserId_norm\", dropna=False)\n",
    "        .agg(Age=(\"Age\", mode_or_first), Gender=(\"Gender\", mode_or_first))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return c_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acd90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Customer normalization & collapse\n",
    "# Normalize ids, pick Age/Gender variants, and collapse duplicates.\n",
    "\n",
    "# %%\n",
    "def _prep_customers(customers: pd.DataFrame) -> pd.DataFrame:\n",
    "    c = customers.copy()\n",
    "    c[\"shopUserId_norm\"] = _norm_id(c[\"shopUserId\"])\n",
    "    c[\"Age\"] = pd.to_numeric(_pick(c, [\"Age\", \"age\", \"customer_age\"]), errors=\"coerce\").astype(\"Float64\")\n",
    "    c[\"Gender\"] = _pick(c, [\"Gender\", \"gender\", \"customer_gender\"])\n",
    "    c_agg = (\n",
    "        c.groupby(\"shopUserId_norm\", dropna=False)\n",
    "        .agg(Age=(\"Age\", mode_or_first), Gender=(\"Gender\", mode_or_first))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return c_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6110ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Merge tx + customers\n",
    "# Attach age/gender and apply normalization.\n",
    "\n",
    "# %%\n",
    "def _merge_tx_customers(base_df: pd.DataFrame, c_agg: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = base_df.merge(c_agg, left_on=\"shopUserId\", right_on=\"shopUserId_norm\", how=\"left\")\n",
    "    df.drop(columns=[\"shopUserId_norm\"], inplace=True)\n",
    "\n",
    "    df[\"age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\").astype(\"Float64\")\n",
    "    df[\"gender\"] = _norm_gender(df[\"Gender\"])\n",
    "    df.drop(columns=[\"Age\", \"Gender\"], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68331f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Optional backfill of country from customers\n",
    "# If tx country is Unknown, pull from customers.Country when available.\n",
    "\n",
    "# %%\n",
    "def _backfill_country_from_customers(df: pd.DataFrame, customers: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"Country\" in customers.columns and (df[\"country\"] == \"Unknown\").any():\n",
    "        country_map = (\n",
    "            customers.assign(shopUserId_norm=_norm_id(customers[\"shopUserId\"]))\n",
    "            .dropna(subset=[\"shopUserId_norm\"])\n",
    "            .drop_duplicates(\"shopUserId_norm\")\n",
    "            .set_index(\"shopUserId_norm\")[\"Country\"]\n",
    "            .astype(\"string[python]\")\n",
    "            .str.strip()\n",
    "        )\n",
    "        mask = df[\"country\"].eq(\"Unknown\")\n",
    "        df.loc[mask, \"country\"] = df.loc[mask, \"shopUserId\"].map(country_map).fillna(\"Unknown\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77fe78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Splitter\n",
    "# Produce a dict of country → dataframe restricted to NORDICS.\n",
    "\n",
    "# %%\n",
    "def split_nordics(tx: pd.DataFrame, customers: pd.DataFrame) -> dict:\n",
    "    base_df = _build_base_tx(tx)\n",
    "    c_agg = _prep_customers(customers)\n",
    "    df = _merge_tx_customers(base_df, c_agg)\n",
    "    df = _backfill_country_from_customers(df, customers)\n",
    "    return {cname: df[df[\"country\"] == cname].copy() for cname in NORDICS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0da06a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export helpers: totals & aggregates\n",
    "# Small builders used by the JSON exporter.\n",
    "# (UPDATED: added _agg_city_monthly)\n",
    "\n",
    "# %%\n",
    "def _country_totals(df: pd.DataFrame):\n",
    "    total_revenue = int(np.rint(df[\"rev\"].sum()))\n",
    "    customers_cnt = int(df[\"shopUserId\"].nunique())\n",
    "    total_orders = int(df[\"orderId\"].nunique())\n",
    "    aov_country = None if total_orders == 0 else int(round(float(total_revenue) / total_orders))\n",
    "    return total_revenue, customers_cnt, total_orders, aov_country\n",
    "\n",
    "def _agg_city(df: pd.DataFrame):\n",
    "    agg_city = (\n",
    "        df.groupby(\"city\", dropna=False, sort=False)\n",
    "        .agg(total_revenue_sek=(\"rev\", \"sum\"), customers_count=(\"shopUserId\", \"nunique\"))\n",
    "    )\n",
    "    agg_city[\"total_revenue_sek\"] = np.rint(agg_city[\"total_revenue_sek\"]).astype(\"int64\")\n",
    "    agg_city[\"customers_count\"] = agg_city[\"customers_count\"].astype(\"int64\")\n",
    "    city_orders = df.groupby(\"city\", dropna=False)[\"orderId\"].nunique().rename(\"total_orders\").astype(\"int64\")\n",
    "    return agg_city, city_orders\n",
    "\n",
    "def _agg_customer(df: pd.DataFrame):\n",
    "    df_cust = df[df[\"shopUserId\"].notna()].copy()\n",
    "    agg_customer = (\n",
    "        df_cust.groupby([\"city\", \"shopUserId\"], dropna=False, sort=False)\n",
    "        .agg(\n",
    "            total_spent_sek=(\"rev\", \"sum\"),\n",
    "            total_orders=(\"orderId\", \"nunique\"),\n",
    "            first_order=(\"created\", \"min\"),\n",
    "            last_order=(\"created\", \"max\"),\n",
    "            age=(\"age\", mode_or_first),\n",
    "            gender=(\"gender\", mode_or_first),\n",
    "        )\n",
    "    )\n",
    "    agg_customer[\"total_spent_sek\"] = np.rint(agg_customer[\"total_spent_sek\"]).astype(\"int64\")\n",
    "    agg_customer = agg_customer.sort_index(level=[\"city\", \"shopUserId\"])\n",
    "    return agg_customer\n",
    "\n",
    "def _agg_order(df: pd.DataFrame):\n",
    "    df_cust = df[df[\"shopUserId\"].notna()].copy()\n",
    "    agg_order = (\n",
    "        df_cust.groupby([\"city\", \"shopUserId\", \"orderId\"], dropna=False, sort=False)\n",
    "        .agg(\n",
    "            order_total_sek=(\"rev\", \"sum\"),\n",
    "            n_items=(\"orderId\", \"size\"),\n",
    "            created=(\"created\", \"min\"),\n",
    "            order_type=(\"type\", mode_or_first),\n",
    "            price=(\"price\", mode_or_first),\n",
    "        )\n",
    "    )\n",
    "    agg_order[\"order_total_sek\"] = np.rint(agg_order[\"order_total_sek\"]).astype(\"int64\")\n",
    "    agg_order = agg_order.sort_index(level=[\"city\", \"shopUserId\", \"orderId\"])\n",
    "    return agg_order\n",
    "\n",
    "def _agg_city_monthly(df: pd.DataFrame) -> dict[str, dict[str, int]]:\n",
    "    \"\"\"\n",
    "    NEW: sum revenue by city and calendar month (YYYY-MM).\n",
    "    Returns {city -> { 'YYYY-MM': total_revenue_sek_int, ... }, ...}\n",
    "    \"\"\"\n",
    "    dfm = df.copy()\n",
    "    # Ensure naive, month string\n",
    "    ym = dfm[\"created\"]\n",
    "    if getattr(ym.dt, \"tz\", None) is not None:\n",
    "        ym = ym.dt.tz_localize(None)\n",
    "    dfm[\"year_month\"] = ym.dt.to_period(\"M\").astype(str)\n",
    "\n",
    "    g = (\n",
    "        dfm.groupby([\"city\", \"year_month\"], dropna=False)[\"rev\"]\n",
    "        .sum()\n",
    "        .round()\n",
    "        .astype(\"int64\")\n",
    "    )\n",
    "    out: dict[str, dict[str, int]] = {}\n",
    "    for (cty, ym), val in g.items():\n",
    "        ckey = \"Unknown\" if pd.isna(cty) else str(cty)\n",
    "        out.setdefault(ckey, {})[ym] = int(val)\n",
    "    return out\n",
    "\n",
    "def _customers_by_channel(df: pd.DataFrame) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count distinct customers per 'type' (channel).\n",
    "    Assumes 'type' is always one of: 'telephone', 'web', or 'Email'.\n",
    "    \"\"\"\n",
    "    if \"type\" not in df.columns:\n",
    "        return {}\n",
    "    tmp = df[[\"shopUserId\", \"type\"]].dropna().copy()\n",
    "\n",
    "    # No normalization needed; just use the type as channel\n",
    "    tmp[\"channel\"] = tmp[\"type\"]\n",
    "\n",
    "    # distinct customers per channel\n",
    "    g = (\n",
    "        tmp.dropna(subset=[\"channel\", \"shopUserId\"])\n",
    "           .groupby(\"channel\")[\"shopUserId\"]\n",
    "           .nunique()\n",
    "           .astype(int)\n",
    "    )\n",
    "    return {k: int(v) for k, v in g.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a695f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export helpers: items extraction\n",
    "# Build items_df from original tx aligned to country df indices.\n",
    "\n",
    "# %%\n",
    "def _build_items_grouped(df_country: pd.DataFrame, tx: pd.DataFrame, articles: pd.DataFrame | None = None):\n",
    "    item_cols = [\"sku\",\"groupId\",\"created\",\"quantity\",\"price_sek\",\"name\",\"line_total_sek\",\"type\",\"brand\",\"category\",\"price\"]\n",
    "    present   = [c for c in item_cols if c in tx.columns]\n",
    "\n",
    "    items_df = pd.DataFrame({\"city\": df_country[\"city\"], \"shopUserId\": df_country[\"shopUserId\"], \"orderId\": df_country[\"orderId\"]})\n",
    "    for c in present:\n",
    "        if c == \"created\":\n",
    "            col_created = df_country[\"created\"]\n",
    "            try:\n",
    "                if getattr(col_created.dt, \"tz\", None) is not None:\n",
    "                    col_created = col_created.dt.tz_localize(None)\n",
    "            except Exception:\n",
    "                pass\n",
    "            items_df[c] = col_created\n",
    "        else:\n",
    "            items_df[c] = tx.loc[df_country.index, c] if c in tx.columns else None\n",
    "\n",
    "    # --- NEW: enrich brand/category from articles ---\n",
    "    if articles is not None:\n",
    "        art = articles.copy()\n",
    "\n",
    "        # normalize join keys\n",
    "        for key in (\"sku\", \"groupId\"):\n",
    "            if key in art.columns:\n",
    "                art[key] = art[key].astype(\"string[python]\").str.strip()\n",
    "        if \"sku\" in items_df.columns:\n",
    "            items_df[\"sku\"] = items_df[\"sku\"].astype(\"string[python]\").str.strip()\n",
    "        if \"groupId\" in items_df.columns:\n",
    "            items_df[\"groupId\"] = items_df[\"groupId\"].astype(\"string[python]\").str.strip()\n",
    "\n",
    "        # ensure target columns exist\n",
    "        if \"brand\" not in items_df.columns:\n",
    "            items_df[\"brand\"] = pd.NA\n",
    "        if \"category\" not in items_df.columns:\n",
    "            items_df[\"category\"] = pd.NA\n",
    "\n",
    "        # maps by SKU (preferred)\n",
    "        if \"sku\" in art.columns:\n",
    "            if \"brand\" in art.columns:\n",
    "                sku_brand = art.dropna(subset=[\"sku\"]).drop_duplicates(\"sku\").set_index(\"sku\")[\"brand\"]\n",
    "                items_df[\"brand\"] = items_df[\"brand\"].fillna(items_df.get(\"sku\").map(sku_brand))\n",
    "            if \"category\" in art.columns:\n",
    "                sku_cat = art.dropna(subset=[\"sku\"]).drop_duplicates(\"sku\").set_index(\"sku\")[\"category\"]\n",
    "                items_df[\"category\"] = items_df[\"category\"].fillna(items_df.get(\"sku\").map(sku_cat))\n",
    "\n",
    "        # fallback maps by groupId\n",
    "        if \"groupId\" in art.columns:\n",
    "            if \"brand\" in art.columns:\n",
    "                gid_brand = art.dropna(subset=[\"groupId\"]).drop_duplicates(\"groupId\").set_index(\"groupId\")[\"brand\"]\n",
    "                items_df[\"brand\"] = items_df[\"brand\"].fillna(items_df.get(\"groupId\").map(gid_brand))\n",
    "            if \"category\" in art.columns:\n",
    "                gid_cat = art.dropna(subset=[\"groupId\"]).drop_duplicates(\"groupId\").set_index(\"groupId\")[\"category\"]\n",
    "                items_df[\"category\"] = items_df[\"category\"].fillna(items_df.get(\"groupId\").map(gid_cat))\n",
    "\n",
    "    items_df = items_df[df_country[\"shopUserId\"].notna()].copy()\n",
    "    items_df[\"city\"] = items_df[\"city\"].fillna(\"Unknown\")\n",
    "    return items_df.groupby([\"city\",\"shopUserId\",\"orderId\"], dropna=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export helpers: JSON node builders\n",
    "# Convert rows to JSON-friendly dicts.\n",
    "\n",
    "# %%\n",
    "def _item_dict(row: pd.Series):\n",
    "    cr = row.get(\"created\")\n",
    "    if isinstance(cr, pd.Timestamp):\n",
    "        cr = cr.isoformat(sep=\" \")\n",
    "    def nz(v): return None if pd.isna(v) else v\n",
    "    def to_int(v): return None if pd.isna(v) else int(v)\n",
    "    def to_float(v): return None if pd.isna(v) else float(v)\n",
    "    return {\n",
    "        \"sku\": nz(row.get(\"sku\")),\n",
    "        \"groupId\": nz(row.get(\"groupId\")),\n",
    "        \"created\": nz(cr),\n",
    "        \"quantity\": to_int(row.get(\"quantity\")),\n",
    "        \"price_sek\": to_int(row.get(\"price_sek\")),\n",
    "        \"name\": nz(row.get(\"name\")),\n",
    "        \"line_total_sek\": to_int(row.get(\"line_total_sek\")),\n",
    "        \"type\": nz(row.get(\"type\")),\n",
    "        \"brand\": nz(row.get(\"brand\")),\n",
    "        \"category\": nz(row.get(\"category\")),\n",
    "        \"price\": to_float(row.get(\"price\")),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6a4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Export: main function\n",
    "# Assemble the JSON structure and write to disk.\n",
    "# (UPDATED: inject per-city monthly revenue)\n",
    "\n",
    "# %%\n",
    "def export_country_json(df_country: pd.DataFrame, tx: pd.DataFrame, country_name: str, out_dir=\"/workspace/data/processed\", articles: pd.DataFrame | None = None):\n",
    "    df = df_country.copy()\n",
    "    df[\"city\"] = df[\"city\"].fillna(\"Unknown\")\n",
    "\n",
    "    # --- totals ---\n",
    "    total_revenue, customers_cnt, total_orders, aov_country = _country_totals(df)\n",
    "\n",
    "    # --- city/ customer / order aggregates ---\n",
    "    agg_city, city_orders = _agg_city(df)\n",
    "    agg_customer = _agg_customer(df)\n",
    "    agg_order = _agg_order(df)\n",
    "\n",
    "    # --- monthly revenue per city (existing new feature) ---\n",
    "    city_monthly_map = _agg_city_monthly(df)\n",
    "\n",
    "    # --- NEW: unique customers by channel (from 'type') ---\n",
    "    customers_by_channel = _customers_by_channel(df)\n",
    "\n",
    "    # --- items ---\n",
    "    items_grouped = _build_items_grouped(df_country, tx, articles=articles)  \n",
    "\n",
    "    # ---------- build JSON ----------\n",
    "    top_key = country_name.lower()\n",
    "    result = {\n",
    "        top_key: {\n",
    "            \"total_revenue_sek\": int(total_revenue),\n",
    "            \"customers_count\": int(customers_cnt),\n",
    "            \"total_orders\": int(total_orders),\n",
    "            \"avg_order_value_sek\": aov_country,\n",
    "            \"customers_by_channel\": customers_by_channel,     # << NEW\n",
    "            \"cities\": {},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # cities\n",
    "    for cty, row in agg_city.iterrows():\n",
    "        ckey = \"Unknown\" if pd.isna(cty) else str(cty)\n",
    "        orders_c = int(city_orders.get(cty, 0))\n",
    "        rev_c = int(row[\"total_revenue_sek\"])\n",
    "        aov_c = None if orders_c == 0 else int(round(float(rev_c) / orders_c))\n",
    "\n",
    "        result[top_key][\"cities\"][ckey] = {\n",
    "            \"total_revenue_sek\": rev_c,\n",
    "            \"customers_count\": int(row[\"customers_count\"]),\n",
    "            \"total_orders\": orders_c,\n",
    "            \"avg_order_value_sek\": aov_c,\n",
    "            # NEW: monthly breakdown here\n",
    "            \"monthly_revenue_sek\": city_monthly_map.get(ckey, {}),\n",
    "            \"customers\": {},\n",
    "        }\n",
    "\n",
    "    # customers + orders + items (unchanged)\n",
    "    for (cty, uid), row in agg_customer.iterrows():\n",
    "        status = status_from_orders(int(row[\"total_orders\"]))\n",
    "        first_iso = row[\"first_order\"].isoformat(sep=\" \") if pd.notna(row[\"first_order\"]) else None\n",
    "        last_iso = row[\"last_order\"].isoformat(sep=\" \") if pd.notna(row[\"last_order\"]) else None\n",
    "\n",
    "        age_val = None\n",
    "        if \"age\" in row and pd.notna(row[\"age\"]):\n",
    "            try:\n",
    "                age_val = int(row[\"age\"])\n",
    "            except Exception:\n",
    "                age_val = None\n",
    "        gender_val = None if \"gender\" not in row or pd.isna(row[\"gender\"]) else str(row[\"gender\"])\n",
    "\n",
    "        cust_node = {\n",
    "            \"summary\": {\n",
    "                \"total_orders\": int(row[\"total_orders\"]),\n",
    "                \"total_spent_sek\": int(row[\"total_spent_sek\"]),\n",
    "                \"first_order\": first_iso,\n",
    "                \"last_order\": last_iso,\n",
    "                \"status\": status,\n",
    "                \"age\": age_val,\n",
    "                \"gender\": gender_val,\n",
    "            },\n",
    "            \"orders\": {},\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            cust_orders = agg_order.loc[(cty, uid)]\n",
    "            if isinstance(cust_orders, pd.Series):\n",
    "                cust_orders = cust_orders.to_frame().T\n",
    "            for oid, orow in cust_orders.iterrows():\n",
    "                try:\n",
    "                    items_for_order = items_grouped.get_group((cty, uid, oid))\n",
    "                    items = [_item_dict(r) for _, r in items_for_order.iterrows()]\n",
    "                except KeyError:\n",
    "                    items = []\n",
    "                cust_node[\"orders\"][str(oid)] = {\n",
    "                    \"created\": orow[\"created\"].isoformat(sep=\" \") if pd.notna(orow[\"created\"]) else None,\n",
    "                    \"order_total_sek\": int(orow[\"order_total_sek\"]),\n",
    "                    \"n_items\": int(orow[\"n_items\"]),\n",
    "                    \"order_type\": None if pd.isna(orow[\"order_type\"]) else orow[\"order_type\"],\n",
    "                    \"price\": None if pd.isna(orow.get(\"price\")) else float(orow.get(\"price\")),\n",
    "                    \"items\": items,\n",
    "                }\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        ckey = \"Unknown\" if pd.isna(cty) else str(cty)\n",
    "        result[top_key][\"cities\"][ckey][\"customers\"][str(uid)] = cust_node\n",
    "\n",
    "    # write\n",
    "    out_path = Path(out_dir) / f\"{country_name}.json\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfb6c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /workspace/data/processed/Sweden.json\n",
      "Saved: /workspace/data/processed/Denmark.json\n",
      "Saved: /workspace/data/processed/Finland.json\n",
      "Saved: /workspace/data/processed/Norway.json\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Example usage\n",
    "# Split once, export four times.\n",
    "\n",
    "# %%\n",
    "countries = split_nordics(tx, customers)\n",
    "export_country_json(countries[\"Sweden\"],  tx, \"Sweden\")\n",
    "export_country_json(countries[\"Denmark\"], tx, \"Denmark\")\n",
    "export_country_json(countries[\"Finland\"], tx, \"Finland\")\n",
    "export_country_json(countries[\"Norway\"],  tx, \"Norway\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51f7ae",
   "metadata": {},
   "source": [
    "## Flatten json for quick math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a699804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Imports & Futures\n",
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de4a5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Paths & Config\n",
    "\n",
    "# %%\n",
    "# ---- configure these paths ----\n",
    "INPUT_DIR  = Path(\"../data/processed\")     # Sweden.json, Denmark.json, Finland.json, Norway.json\n",
    "OUTPUT_DIR = Path(\"../data/parquet_out\")   # will contain the 5 Parquet files\n",
    "\n",
    "COUNTRY_FILES = {\n",
    "    \"Sweden\":  INPUT_DIR / \"Sweden.json\",\n",
    "    \"Denmark\": INPUT_DIR / \"Denmark.json\",\n",
    "    \"Finland\": INPUT_DIR / \"Finland.json\",\n",
    "    \"Norway\":  INPUT_DIR / \"Norway.json\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c9c8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Stable Column Schemas\n",
    "# (UPDATED: added CS_COUNTRY_CHANNEL)\n",
    "\n",
    "# %%\n",
    "CS_COUNTRY = [\"country\",\"total_revenue_sek\",\"customers_count\",\"total_orders\",\"avg_order_value_sek\"]\n",
    "CS_CITY    = [\"country\",\"city\",\"total_revenue_sek\",\"customers_count\",\"total_orders\",\"avg_order_value_sek\"]\n",
    "\n",
    "# Added age & gender here\n",
    "CS_CUST    = [\"country\",\"city\",\"customer_id\",\"total_orders\",\"total_spent_sek\",\n",
    "              \"first_order\",\"last_order\",\"status\",\"age\",\"gender\"]\n",
    "\n",
    "CS_ORDERS  = [\"country\",\"city\",\"customer_id\",\"order_id\",\"created\",\"order_total_sek\",\"n_items\",\"order_type\",\"price\"]\n",
    "CS_ITEMS   = [\"country\",\"city\",\"customer_id\",\"order_id\",\"sku\",\"groupId\",\"created\",\"quantity\",\"price_sek\",\"name\",\"line_total_sek\",\"type\",\"brand\",\"category\",\"price\"]\n",
    "\n",
    "# monthly revenue per city (existing)\n",
    "CS_CITY_MONTHLY = [\"country\",\"city\",\"year_month\",\"total_revenue_sek\"]\n",
    "\n",
    "# NEW: customers by channel per country\n",
    "CS_COUNTRY_CHANNEL = [\"country\",\"channel\",\"customers_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b4054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # I/O Helpers\n",
    "\n",
    "# %%\n",
    "def load_json(path: Path) -> dict:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca5da16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # DataFrame Utilities\n",
    "\n",
    "# %%\n",
    "def _ensure(df: pd.DataFrame | None, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Ensure columns exist, preserve dtypes, and reorder.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame({c: pd.Series([], dtype=\"object\") for c in cols})[cols]\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    return df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13e7244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # JSON Shape Helpers\n",
    "\n",
    "# %%\n",
    "def _unwrap(obj: dict, hint: str) -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Accept {\"denmark\": {...}} or {...}.\n",
    "    Returns (country_name_capitalized, payload_dict).\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict) and len(obj) == 1 and isinstance(next(iter(obj.values())), dict):\n",
    "        k = next(iter(obj.keys()))\n",
    "        return k.capitalize(), next(iter(obj.values()))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Top-level JSON must be an object/dict\")\n",
    "    return hint, obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2609f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # JSON Shape Helpers\n",
    "\n",
    "# %%\n",
    "def _unwrap(obj: dict, hint: str) -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Accept {\"denmark\": {...}} or {...}.\n",
    "    Returns (country_name_capitalized, payload_dict).\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict) and len(obj) == 1 and isinstance(next(iter(obj.values())), dict):\n",
    "        k = next(iter(obj.keys()))\n",
    "        return k.capitalize(), next(iter(obj.values()))\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Top-level JSON must be an object/dict\")\n",
    "    return hint, obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad3680b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Flatten: Country → Row Buckets\n",
    "# (UPDATED: collect \"country_channels\" from root.customers_by_channel)\n",
    "\n",
    "# %%\n",
    "def flatten_country(obj: dict, country_hint: str) -> dict[str, list[dict]]:\n",
    "    country, root = _unwrap(obj, country_hint)\n",
    "    out = {\n",
    "        \"country_summary\": [{\n",
    "            \"country\": country,\n",
    "            \"total_revenue_sek\": root.get(\"total_revenue_sek\"),\n",
    "            \"customers_count\": root.get(\"customers_count\"),\n",
    "            \"total_orders\": root.get(\"total_orders\"),\n",
    "            \"avg_order_value_sek\": root.get(\"avg_order_value_sek\"),\n",
    "        }],\n",
    "        \"country_channels\": [],   # NEW\n",
    "        \"city_summary\": [],\n",
    "        \"city_monthly\": [],\n",
    "        \"customer_summary\": [],\n",
    "        \"orders\": [],\n",
    "        \"order_items\": [],\n",
    "    }\n",
    "\n",
    "    # NEW: expand customers_by_channel dict into rows\n",
    "    for ch, cnt in (root.get(\"customers_by_channel\") or {}).items():\n",
    "        out[\"country_channels\"].append({\n",
    "            \"country\": country,\n",
    "            \"channel\": ch,\n",
    "            \"customers_count\": cnt,\n",
    "        })\n",
    "\n",
    "\n",
    "    for city, cnode in (root.get(\"cities\") or {}).items():\n",
    "        out[\"city_summary\"].append({\n",
    "            \"country\": country, \"city\": city,\n",
    "            \"total_revenue_sek\": cnode.get(\"total_revenue_sek\"),\n",
    "            \"customers_count\": cnode.get(\"customers_count\"),\n",
    "            \"total_orders\": cnode.get(\"total_orders\"),\n",
    "            \"avg_order_value_sek\": cnode.get(\"avg_order_value_sek\"),\n",
    "        })\n",
    "\n",
    "        # NEW: monthly map { 'YYYY-MM': revenue }\n",
    "        for ym, rev in (cnode.get(\"monthly_revenue_sek\") or {}).items():\n",
    "            out[\"city_monthly\"].append({\n",
    "                \"country\": country,\n",
    "                \"city\": city,\n",
    "                \"year_month\": ym,\n",
    "                \"total_revenue_sek\": rev,\n",
    "            })\n",
    "\n",
    "        for cust_id, cst in (cnode.get(\"customers\") or {}).items():\n",
    "            summ = cst.get(\"summary\") or {}\n",
    "            out[\"customer_summary\"].append({\n",
    "                \"country\": country,\n",
    "                \"city\": city,\n",
    "                \"customer_id\": cust_id,\n",
    "                \"total_orders\": summ.get(\"total_orders\"),\n",
    "                \"total_spent_sek\": summ.get(\"total_spent_sek\"),\n",
    "                \"first_order\": summ.get(\"first_order\"),\n",
    "                \"last_order\": summ.get(\"last_order\"),\n",
    "                \"status\": summ.get(\"status\"),\n",
    "                \"age\": summ.get(\"age\"),\n",
    "                \"gender\": summ.get(\"gender\"),\n",
    "            })\n",
    "            for order_id, ordn in (cst.get(\"orders\") or {}).items():\n",
    "                out[\"orders\"].append({\n",
    "                    \"country\": country, \"city\": city, \"customer_id\": cust_id, \"order_id\": order_id,\n",
    "                    \"created\": ordn.get(\"created\"),\n",
    "                    \"order_total_sek\": ordn.get(\"order_total_sek\"),\n",
    "                    \"n_items\": ordn.get(\"n_items\"),\n",
    "                    \"order_type\": ordn.get(\"order_type\"),\n",
    "                    \"price\": ordn.get(\"price\"),\n",
    "                })\n",
    "                for it in (ordn.get(\"items\") or []):\n",
    "                    out[\"order_items\"].append({\n",
    "                        \"country\": country, \"city\": city, \"customer_id\": cust_id, \"order_id\": order_id,\n",
    "                        \"sku\": it.get(\"sku\"), \"groupId\": it.get(\"groupId\"), \"created\": it.get(\"created\"),\n",
    "                        \"quantity\": it.get(\"quantity\"), \"price_sek\": it.get(\"price_sek\"), \"name\": it.get(\"name\"),\n",
    "                        \"line_total_sek\": it.get(\"line_total_sek\"), \"type\": it.get(\"type\"),\n",
    "                        \"brand\": it.get(\"brand\"), \"category\": it.get(\"category\"), \"price\": it.get(\"price\"),\n",
    "                    })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bc676bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Aggregation Orchestrator\n",
    "# (UPDATED: include \"city_monthly\" bucket)\n",
    "\n",
    "# %%\n",
    "def collect_buckets(country_files: dict[str, Path]) -> dict[str, list[dict]]:\n",
    "    buckets = {k: [] for k in [\n",
    "        \"country_summary\",\"country_channels\",\"city_summary\",\"city_monthly\",\"customer_summary\",\"orders\",\"order_items\"\n",
    "    ]}\n",
    "    for name, path in country_files.items():\n",
    "        if not path.exists():\n",
    "            print(f\"[warn] missing: {path}\")\n",
    "            continue\n",
    "        rows = flatten_country(load_json(path), name)\n",
    "        for k, v in rows.items():\n",
    "            buckets[k].extend(v)\n",
    "        print(f\"[ok] parsed {name}\")\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac9c7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Materialize DataFrames (Fixed Schemas)\n",
    "# (UPDATED: return df_city_monthly)\n",
    "\n",
    "# %%\n",
    "def to_dataframes(buckets: dict[str, list[dict]]) -> dict[str, pd.DataFrame]:\n",
    "    df_country = _ensure(pd.DataFrame(buckets[\"country_summary\"]),   CS_COUNTRY)\n",
    "    df_cc      = _ensure(pd.DataFrame(buckets[\"country_channels\"]),  CS_COUNTRY_CHANNEL)  # NEW\n",
    "    df_city    = _ensure(pd.DataFrame(buckets[\"city_summary\"]),      CS_CITY)\n",
    "    df_city_m  = _ensure(pd.DataFrame(buckets[\"city_monthly\"]),      CS_CITY_MONTHLY)\n",
    "    df_cust    = _ensure(pd.DataFrame(buckets[\"customer_summary\"]),  CS_CUST)\n",
    "    df_orders  = _ensure(pd.DataFrame(buckets[\"orders\"]),            CS_ORDERS)\n",
    "    df_items   = _ensure(pd.DataFrame(buckets[\"order_items\"]),       CS_ITEMS)\n",
    "    return {\n",
    "        \"country_summary\": df_country,\n",
    "        \"country_channels\": df_cc,         # NEW\n",
    "        \"city_summary\": df_city,\n",
    "        \"city_monthly\": df_city_m,\n",
    "        \"customer_summary\": df_cust,\n",
    "        \"orders\": df_orders,\n",
    "        \"order_items\": df_items,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bd943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d74eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Persist to Parquet\n",
    "\n",
    "# %%\n",
    "def write_all_parquet(dfs: dict[str, pd.DataFrame], out_dir: Path) -> None:\n",
    "    save_parquet(dfs[\"country_summary\"],   out_dir / \"country_summary.parquet\")\n",
    "    save_parquet(dfs[\"country_channels\"],  out_dir / \"country_customers_by_channel.parquet\")  # NEW\n",
    "    save_parquet(dfs[\"city_summary\"],      out_dir / \"city_summary.parquet\")\n",
    "    save_parquet(dfs[\"city_monthly\"],      out_dir / \"city_monthly_revenue.parquet\")\n",
    "    save_parquet(dfs[\"customer_summary\"],  out_dir / \"customer_summary.parquet\")\n",
    "    save_parquet(dfs[\"orders\"],            out_dir / \"orders.parquet\")\n",
    "    save_parquet(dfs[\"order_items\"],       out_dir / \"order_items.parquet\")\n",
    "    print(f\"[done] wrote Parquet files to {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5932b3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] parsed Sweden\n",
      "[ok] parsed Denmark\n",
      "[ok] parsed Finland\n",
      "[ok] parsed Norway\n",
      "[done] wrote Parquet files to /workspace/data/parquet_out\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Main\n",
    "\n",
    "# %%\n",
    "def main():\n",
    "    buckets = collect_buckets(COUNTRY_FILES)\n",
    "    dfs = to_dataframes(buckets)\n",
    "    write_all_parquet(dfs, OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b327b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>sku</th>\n",
       "      <th>groupId</th>\n",
       "      <th>created</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_sek</th>\n",
       "      <th>name</th>\n",
       "      <th>line_total_sek</th>\n",
       "      <th>type</th>\n",
       "      <th>brand</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176301</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Varkaus</td>\n",
       "      <td>408235</td>\n",
       "      <td>588421</td>\n",
       "      <td>200304</td>\n",
       "      <td>200304</td>\n",
       "      <td>2025-02-18 08:54:06</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>Laukku tummanharmaa/luonnonvalkoinen</td>\n",
       "      <td>54</td>\n",
       "      <td>telephone</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Accessoarer</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269858</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Tynset</td>\n",
       "      <td>215050</td>\n",
       "      <td>650036</td>\n",
       "      <td>261318-E085</td>\n",
       "      <td>261318</td>\n",
       "      <td>2025-04-03 19:40:49</td>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>Sports-bh</td>\n",
       "      <td>481</td>\n",
       "      <td>web</td>\n",
       "      <td>Glamorise</td>\n",
       "      <td>Sport-bh,Bh utan bygel,Bh,Underkläder</td>\n",
       "      <td>509.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176514</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Forssa</td>\n",
       "      <td>769425</td>\n",
       "      <td>601632</td>\n",
       "      <td>200267</td>\n",
       "      <td>200267</td>\n",
       "      <td>2025-02-26 13:00:28</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>Laukku</td>\n",
       "      <td>32</td>\n",
       "      <td>telephone</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293162</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Hyen</td>\n",
       "      <td>772632</td>\n",
       "      <td>632550</td>\n",
       "      <td>200267</td>\n",
       "      <td>200267</td>\n",
       "      <td>2025-03-21 08:36:45</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>Veske</td>\n",
       "      <td>27</td>\n",
       "      <td>letter</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197971</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Savikylä</td>\n",
       "      <td>320113</td>\n",
       "      <td>231990</td>\n",
       "      <td>261634-3639</td>\n",
       "      <td>261637</td>\n",
       "      <td>2024-07-28 22:16:40</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>Nilkkasukka VID</td>\n",
       "      <td>288</td>\n",
       "      <td>web</td>\n",
       "      <td>Locköstrumpan</td>\n",
       "      <td>Stödstrumpor,Strumpor,Underkläder</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82376</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Billdal</td>\n",
       "      <td>826543</td>\n",
       "      <td>791858</td>\n",
       "      <td>210729-4446</td>\n",
       "      <td>210729</td>\n",
       "      <td>2025-08-12 10:38:48</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>Stickad kofta</td>\n",
       "      <td>169</td>\n",
       "      <td>telephone</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Överdelar,Tröjor</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55646</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Bagarmossen</td>\n",
       "      <td>855266</td>\n",
       "      <td>745073</td>\n",
       "      <td>260701-C080</td>\n",
       "      <td>260701</td>\n",
       "      <td>2025-06-24 13:25:01</td>\n",
       "      <td>1</td>\n",
       "      <td>1223</td>\n",
       "      <td>Korsett Fiore</td>\n",
       "      <td>1223</td>\n",
       "      <td>telephone</td>\n",
       "      <td>Anita</td>\n",
       "      <td>Bh utan bygel,Korsetter,Underkläder</td>\n",
       "      <td>1223.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223690</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Moss</td>\n",
       "      <td>371272</td>\n",
       "      <td>814522</td>\n",
       "      <td>230047-4042</td>\n",
       "      <td>230047</td>\n",
       "      <td>2025-08-29 12:07:20</td>\n",
       "      <td>1</td>\n",
       "      <td>470</td>\n",
       "      <td>Kjole</td>\n",
       "      <td>470</td>\n",
       "      <td>web</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Klänningar,Överdelar,Tunikor</td>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261561</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Moelv</td>\n",
       "      <td>876922</td>\n",
       "      <td>248271</td>\n",
       "      <td>210189-4446</td>\n",
       "      <td>210186</td>\n",
       "      <td>2024-08-09 08:52:19</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>Polojumper</td>\n",
       "      <td>206</td>\n",
       "      <td>telephone</td>\n",
       "      <td>Åshild</td>\n",
       "      <td>Toppar,Överdelar</td>\n",
       "      <td>218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26813</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Köping</td>\n",
       "      <td>643454</td>\n",
       "      <td>587466</td>\n",
       "      <td>293647</td>\n",
       "      <td>293647</td>\n",
       "      <td>2025-02-17 12:57:30</td>\n",
       "      <td>1</td>\n",
       "      <td>1098</td>\n",
       "      <td>Fotvagga</td>\n",
       "      <td>1098</td>\n",
       "      <td>web</td>\n",
       "      <td>Good Living</td>\n",
       "      <td>Fotvård,Vardagshjälpmedel</td>\n",
       "      <td>1098.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country         city customer_id order_id          sku groupId  \\\n",
       "176301  Finland      Varkaus      408235   588421       200304  200304   \n",
       "269858   Norway       Tynset      215050   650036  261318-E085  261318   \n",
       "176514  Finland       Forssa      769425   601632       200267  200267   \n",
       "293162   Norway         Hyen      772632   632550       200267  200267   \n",
       "197971  Finland     Savikylä      320113   231990  261634-3639  261637   \n",
       "82376    Sweden      Billdal      826543   791858  210729-4446  210729   \n",
       "55646    Sweden  Bagarmossen      855266   745073  260701-C080  260701   \n",
       "223690   Norway         Moss      371272   814522  230047-4042  230047   \n",
       "261561   Norway        Moelv      876922   248271  210189-4446  210186   \n",
       "26813    Sweden       Köping      643454   587466       293647  293647   \n",
       "\n",
       "                    created  quantity  price_sek  \\\n",
       "176301  2025-02-18 08:54:06         1         54   \n",
       "269858  2025-04-03 19:40:49         1        481   \n",
       "176514  2025-02-26 13:00:28         1         32   \n",
       "293162  2025-03-21 08:36:45         1         27   \n",
       "197971  2024-07-28 22:16:40         4         72   \n",
       "82376   2025-08-12 10:38:48         1        169   \n",
       "55646   2025-06-24 13:25:01         1       1223   \n",
       "223690  2025-08-29 12:07:20         1        470   \n",
       "261561  2024-08-09 08:52:19         1        206   \n",
       "26813   2025-02-17 12:57:30         1       1098   \n",
       "\n",
       "                                         name  line_total_sek       type  \\\n",
       "176301  Laukku tummanharmaa/luonnonvalkoinen               54  telephone   \n",
       "269858                              Sports-bh             481        web   \n",
       "176514                                 Laukku              32  telephone   \n",
       "293162                                  Veske              27     letter   \n",
       "197971                        Nilkkasukka VID             288        web   \n",
       "82376                          Stickad kofta              169  telephone   \n",
       "55646                           Korsett Fiore            1223  telephone   \n",
       "223690                                  Kjole             470        web   \n",
       "261561                             Polojumper             206  telephone   \n",
       "26813                                Fotvagga            1098        web   \n",
       "\n",
       "                brand                               category   price  \n",
       "176301         Åshild                            Accessoarer     4.9  \n",
       "269858      Glamorise  Sport-bh,Bh utan bygel,Bh,Underkläder   509.0  \n",
       "176514        unknown                                unknown     2.9  \n",
       "293162        unknown                                unknown    29.0  \n",
       "197971  Locköstrumpan      Stödstrumpor,Strumpor,Underkläder     6.5  \n",
       "82376          Åshild                       Överdelar,Tröjor   169.0  \n",
       "55646           Anita    Bh utan bygel,Korsetter,Underkläder  1223.2  \n",
       "223690         Åshild           Klänningar,Överdelar,Tunikor   498.0  \n",
       "261561         Åshild                       Toppar,Överdelar   218.0  \n",
       "26813     Good Living              Fotvård,Vardagshjälpmedel  1098.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"/workspace/data/parquet_out/\")\n",
    "\n",
    "order_items_df = pd.read_parquet(OUTPUT_DIR / \"order_items.parquet\")\n",
    "order_items_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed2682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_categories_by_season.parquet\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Top Categories by Season\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def _season_from_month(m: int) -> str:\n",
    "    # Winter: Dec–Feb, Spring: Mar–May, Summer: Jun–Aug, Autumn: Sep–Nov\n",
    "    if m in (12, 1, 2):   return \"Winter\"\n",
    "    if m in (3, 4, 5):    return \"Spring\"\n",
    "    if m in (6, 7, 8):    return \"Summer\"\n",
    "    return \"Autumn\"\n",
    "\n",
    "def build_top_categories_by_season(\n",
    "    df_items: pd.DataFrame,\n",
    "    category_sep: str = \",\",\n",
    "    keep_top_n: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    df = df_items.copy()\n",
    "\n",
    "    # --- types & defaults ---\n",
    "    df[\"created\"]  = pd.to_datetime(df[\"created\"], errors=\"coerce\")\n",
    "    df[\"quantity\"] = pd.to_numeric(df.get(\"quantity\"), errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "\n",
    "    def _norm(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(\"string[python]\")\n",
    "        s = s.where(~s.isna(), \"Unknown\")\n",
    "        return s.str.strip().fillna(\"Unknown\")\n",
    "\n",
    "    df[\"country\"]  = _norm(df.get(\"country\"))\n",
    "    df[\"category\"] = _norm(df.get(\"category\"))\n",
    "\n",
    "    # --- season label (Dec belongs to next winter) ---\n",
    "    m = df[\"created\"].dt.month\n",
    "    y = df[\"created\"].dt.year\n",
    "    season = m.map(_season_from_month).fillna(\"Unknown\")\n",
    "    season_year = y + (m == 12).astype(int)\n",
    "    df[\"season_label\"] = (season + \" \" + season_year.astype(str)).astype(\"string[python]\")\n",
    "\n",
    "    # --- explode categories (no sharing → integer counts) ---\n",
    "    cat_lists = (\n",
    "        df[\"category\"]\n",
    "          .str.split(category_sep)\n",
    "          .apply(lambda parts: list(dict.fromkeys([p.strip() for p in (parts or []) if p and p.strip()])))\n",
    "    )\n",
    "    df_exp = df.loc[cat_lists.index, [\"country\",\"season_label\",\"quantity\"]].copy()\n",
    "    df_exp[\"category\"] = cat_lists\n",
    "    df_exp = df_exp.explode(\"category\", ignore_index=True)\n",
    "    df_exp[\"category\"] = _norm(df_exp[\"category\"])\n",
    "\n",
    "    # --- aggregate & rank ---\n",
    "    agg = (\n",
    "        df_exp.groupby([\"country\",\"season_label\",\"category\"], dropna=False)[\"quantity\"]\n",
    "              .sum().reset_index(name=\"count\")\n",
    "    )\n",
    "    agg[\"count\"] = agg[\"count\"].astype(\"int64\")\n",
    "\n",
    "    agg = agg.sort_values([\"country\",\"season_label\",\"count\"], ascending=[True, True, False])\n",
    "    agg[\"rank\"] = (\n",
    "        agg.groupby([\"country\",\"season_label\"])[\"count\"]\n",
    "           .rank(method=\"first\", ascending=False).astype(int)\n",
    "    )\n",
    "\n",
    "    top = (\n",
    "        agg[agg[\"rank\"] <= keep_top_n]\n",
    "        .sort_values([\"country\",\"season_label\",\"rank\"], ascending=[True, True, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # final tidy schema\n",
    "    return top[[\"country\",\"season_label\",\"category\",\"count\",\"rank\"]]\n",
    "\n",
    "# --- usage ---\n",
    "items_path = OUTPUT_DIR / \"order_items.parquet\"\n",
    "top_path   = OUTPUT_DIR / \"top_categories_by_season.parquet\"\n",
    "df_items = pd.read_parquet(items_path)\n",
    "df_top = build_top_categories_by_season(df_items)\n",
    "df_top.to_parquet(top_path, index=False)\n",
    "print(f\"[done] wrote {top_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29cb560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_groupids_by_season.parquet\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Top Products by Season\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def _season_from_month(m: int) -> str:\n",
    "    # Winter: Dec–Feb, Spring: Mar–May, Summer: Jun–Aug, Autumn: Sep–Nov\n",
    "    if m in (12, 1, 2):   return \"Winter\"\n",
    "    if m in (3, 4, 5):    return \"Spring\"\n",
    "    if m in (6, 7, 8):    return \"Summer\"\n",
    "    return \"Autumn\"\n",
    "\n",
    "def build_top_groupids_by_season(\n",
    "    df_items: pd.DataFrame,\n",
    "    keep_top_n: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    df = df_items.copy()\n",
    "\n",
    "    # --- types & defaults ---\n",
    "    df[\"created\"]  = pd.to_datetime(df[\"created\"], errors=\"coerce\")\n",
    "    df[\"quantity\"] = pd.to_numeric(df.get(\"quantity\"), errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "\n",
    "    def _norm(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(\"string[python]\")\n",
    "        s = s.where(~s.isna(), \"Unknown\")\n",
    "        return s.str.strip().fillna(\"Unknown\")\n",
    "\n",
    "    df[\"country\"] = _norm(df.get(\"country\"))\n",
    "    df[\"groupId\"] = _norm(df.get(\"groupId\"))\n",
    "\n",
    "    # choose a product-name column if present; normalize\n",
    "    if   \"name\" in df.columns:           base_name = df[\"name\"]\n",
    "    elif \"productName\" in df.columns:     base_name = df[\"productName\"]\n",
    "    elif \"title\" in df.columns:           base_name = df[\"title\"]\n",
    "    elif \"product_name\" in df.columns:    base_name = df[\"product_name\"]\n",
    "    else:                                 base_name = pd.Series(pd.NA, index=df.index)\n",
    "    df[\"name\"] = _norm(base_name)\n",
    "\n",
    "    # --- season label (Dec → next winter) ---\n",
    "    m = df[\"created\"].dt.month\n",
    "    y = df[\"created\"].dt.year\n",
    "    season = m.map(_season_from_month).fillna(\"Unknown\")\n",
    "    season_year = y + (m == 12).astype(int)\n",
    "    df[\"season_label\"] = (season + \" \" + season_year.astype(str)).astype(\"string[python]\")\n",
    "\n",
    "    # --- aggregate counts by groupId ---\n",
    "    gid_agg = (\n",
    "        df.groupby([\"country\",\"season_label\",\"groupId\"], dropna=False)[\"quantity\"]\n",
    "          .sum()\n",
    "          .reset_index(name=\"count\")\n",
    "    )\n",
    "    gid_agg[\"count\"] = gid_agg[\"count\"].astype(\"int64\")\n",
    "\n",
    "    # representative name per (country, season_label, groupId) by max quantity (ties -> alphabetical)\n",
    "    rep_name = (\n",
    "        df.groupby([\"country\",\"season_label\",\"groupId\",\"name\"], dropna=False)[\"quantity\"]\n",
    "          .sum().reset_index(name=\"qty\")\n",
    "          .sort_values([\"country\",\"season_label\",\"groupId\",\"qty\",\"name\"],\n",
    "                       ascending=[True, True, True, False, True])\n",
    "          .drop_duplicates([\"country\",\"season_label\",\"groupId\"])\n",
    "          .rename(columns={\"name\":\"rep_name\"})\n",
    "          [[\"country\",\"season_label\",\"groupId\",\"rep_name\"]]\n",
    "    )\n",
    "    gid_agg = gid_agg.merge(rep_name, on=[\"country\",\"season_label\",\"groupId\"], how=\"left\")\n",
    "\n",
    "    # --- rank within (country, season) and take top N ---\n",
    "    gid_agg = gid_agg.sort_values([\"country\",\"season_label\",\"count\"], ascending=[True, True, False])\n",
    "    gid_agg[\"rank\"] = (\n",
    "        gid_agg.groupby([\"country\",\"season_label\"])[\"count\"]\n",
    "               .rank(method=\"first\", ascending=False).astype(int)\n",
    "    )\n",
    "    top = (\n",
    "        gid_agg[gid_agg[\"rank\"] <= keep_top_n]\n",
    "        .sort_values([\"country\",\"season_label\",\"rank\"])\n",
    "        .rename(columns={\"groupId\":\"value\", \"rep_name\":\"name\"})\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # final schema\n",
    "    return top[[\"country\",\"season_label\",\"value\",\"name\",\"count\",\"rank\"]]\n",
    "\n",
    "\n",
    "# --- usage ---\n",
    "items_path = OUTPUT_DIR / \"order_items.parquet\"\n",
    "top_path   = OUTPUT_DIR / \"top_groupids_by_season.parquet\"\n",
    "df_items = pd.read_parquet(items_path)\n",
    "df_top = build_top_groupids_by_season(df_items)\n",
    "df_top.to_parquet(top_path, index=False)\n",
    "print(f\"[done] wrote {top_path.resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaebd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Top Repurchased Products by Country\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_top_repurchase_groupids_by_country_unique_days(\n",
    "    df_items: pd.DataFrame,\n",
    "    unique_days_threshold: int = 4,   # \"more than 4 different dates\"\n",
    "    keep_top_n: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    df = df_items.copy()\n",
    "\n",
    "    # --- types & normalization ---\n",
    "    df[\"created\"] = pd.to_datetime(df.get(\"created\"), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"created\"])  # need a date to test \"different dates\"\n",
    "    df[\"purchase_date\"] = df[\"created\"].dt.date\n",
    "    df[\"quantity\"] = pd.to_numeric(df.get(\"quantity\"), errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "\n",
    "    def _norm(s: pd.Series) -> pd.Series:\n",
    "        s = s.astype(\"string[python]\")\n",
    "        s = s.where(~s.isna(), \"Unknown\")\n",
    "        return s.str.strip().fillna(\"Unknown\")\n",
    "\n",
    "    df[\"country\"]     = _norm(df.get(\"country\"))\n",
    "    df[\"groupId\"]     = _norm(df.get(\"groupId\"))\n",
    "    df[\"customer_id\"] = _norm(df.get(\"customer_id\"))\n",
    "\n",
    "    # best-available product name column\n",
    "    if   \"name\" in df.columns:          base_name = df[\"name\"]\n",
    "    elif \"productName\" in df.columns:   base_name = df[\"productName\"]\n",
    "    elif \"title\" in df.columns:         base_name = df[\"title\"]\n",
    "    elif \"product_name\" in df.columns:  base_name = df[\"product_name\"]\n",
    "    else:                               base_name = pd.Series(pd.NA, index=df.index)\n",
    "    df[\"name\"] = _norm(base_name)\n",
    "\n",
    "    # ---------- per customer, per (country, groupId): count UNIQUE purchase dates ----------\n",
    "    per_cust = (\n",
    "        df.groupby([\"country\",\"groupId\",\"customer_id\"], dropna=False)\n",
    "          .agg(unique_days=(\"purchase_date\", pd.Series.nunique),\n",
    "               qty=(\"quantity\",\"sum\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    eligible = per_cust[per_cust[\"unique_days\"] > unique_days_threshold].copy()\n",
    "\n",
    "    # ---------- # of repurchasing customers per (country, groupId) ----------\n",
    "    rep_counts = (\n",
    "        eligible.groupby([\"country\",\"groupId\"], dropna=False)[\"customer_id\"]\n",
    "                .nunique().reset_index(name=\"repurchasers\")\n",
    "    ).astype({\"repurchasers\":\"int64\"})\n",
    "\n",
    "    # ---------- representative name per (country, groupId) ----------\n",
    "    # Pick the name with the highest overall quantity (ties → alphabetical)\n",
    "    name_weight = (\n",
    "        df.groupby([\"country\",\"groupId\",\"name\"], dropna=False)[\"quantity\"]\n",
    "          .sum().reset_index(name=\"qty\")\n",
    "    )\n",
    "    rep_name = (\n",
    "        name_weight.sort_values([\"country\",\"groupId\",\"qty\",\"name\"],\n",
    "                                ascending=[True, True, False, True])\n",
    "                   .drop_duplicates([\"country\",\"groupId\"])\n",
    "                   .rename(columns={\"name\":\"rep_name\"})\n",
    "                   [[\"country\",\"groupId\",\"rep_name\"]]\n",
    "    )\n",
    "\n",
    "    out = rep_counts.merge(rep_name, on=[\"country\",\"groupId\"], how=\"left\")\n",
    "\n",
    "    # ---------- rank within country & top-N ----------\n",
    "    out = out.sort_values([\"country\",\"repurchasers\"], ascending=[True, False])\n",
    "    out[\"rank\"] = out.groupby(\"country\")[\"repurchasers\"].rank(method=\"first\", ascending=False).astype(int)\n",
    "\n",
    "    top = (out[out[\"rank\"] <= keep_top_n]\n",
    "           .sort_values([\"country\",\"rank\"])\n",
    "           .rename(columns={\"groupId\":\"value\", \"rep_name\":\"name\"})\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    return top[[\"country\",\"value\",\"name\",\"repurchasers\",\"rank\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3d1a9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_repurchase_groupids_by_country.parquet\n",
      " value                            name  repurchasers  rank\n",
      "261637                 Ankelsokker VID            44     1\n",
      "260695                 Seamless BH-top            38     2\n",
      "260646               Dametrusser 3-pak            23     3\n",
      "263988                Støtteknæstrømpe            17     4\n",
      "261475                       Benklæder            15     5\n",
      "218982                         T-shirt            14     6\n",
      "260513                   BH uden bøjle            14     7\n",
      "260596             BH uden bøjle Stars            14     8\n",
      "210186                      Turtleneck            13     9\n",
      "261610 Stretchtrusser 2-pak Basic Maxi            13    10\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "items_path = Path(\"../data/parquet_out/order_items.parquet\")\n",
    "df_items = pd.read_parquet(items_path)\n",
    "\n",
    "df_top_repurchase_country = build_top_repurchase_groupids_by_country(\n",
    "    df_items,\n",
    "    threshold=4,\n",
    "    keep_top_n=10\n",
    ")\n",
    "\n",
    "out_path = Path(\"../data/parquet_out/top_repurchase_groupids_by_country.parquet\")\n",
    "df_top_repurchase_country.to_parquet(out_path, index=False)\n",
    "print(f\"[done] wrote {out_path.resolve()}\")\n",
    "\n",
    "# quick peek for Denmark\n",
    "print(\n",
    "    df_top_repurchase_country.query(\"country == 'Denmark'\")\n",
    "                             .sort_values(\"rank\")[[\"value\",\"name\",\"repurchasers\",\"rank\"]]\n",
    "                             .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Top Brands by Country\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_top_brands_by_country(df: pd.DataFrame, keep_top_n: int = 10) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\").fillna(1).astype(\"int64\")\n",
    "    df[\"country\"]  = df[\"country\"].astype(\"string\")\n",
    "    df[\"brand\"]    = df[\"brand\"].astype(\"string\").str.strip()\n",
    "\n",
    "    # exclude Unknown/unknown/na/empty and NA values\n",
    "    mask = df[\"brand\"].notna() & ~df[\"brand\"].str.lower().isin({\"unknown\", \"na\", \"\"})\n",
    "    df = df[mask]\n",
    "\n",
    "    agg = (df.groupby([\"country\",\"brand\"], dropna=False)[\"quantity\"]\n",
    "             .sum().reset_index(name=\"count\").astype({\"count\":\"int64\"}))\n",
    "    agg = agg.sort_values([\"country\",\"count\",\"brand\"], ascending=[True, False, True])\n",
    "    agg[\"rank\"] = agg.groupby(\"country\")[\"count\"].rank(method=\"first\", ascending=False).astype(int)\n",
    "\n",
    "    return (agg[agg[\"rank\"] <= keep_top_n]\n",
    "              .sort_values([\"country\",\"rank\"])\n",
    "              .reset_index(drop=True)[[\"country\",\"brand\",\"count\",\"rank\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11a343f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] wrote /workspace/data/parquet_out/top_brands_by_country.parquet\n",
      "        brand  count  rank\n",
      "       Åshild  15409     1\n",
      "       Louise   6434     2\n",
      "    Glamorise   2847     3\n",
      "  Good Living   2485     4\n",
      "    Miss Mary   2300     5\n",
      "       Sloggi   1146     6\n",
      "Locköstrumpan   1054     7\n",
      "        Trofé    886     8\n",
      "     Swegmark    786     9\n",
      "    Funq Wear    753    10\n"
     ]
    }
   ],
   "source": [
    "df_top_brands = build_top_brands_by_country(df_items, keep_top_n=10)\n",
    "\n",
    "out_path = Path(\"../data/parquet_out/top_brands_by_country.parquet\")\n",
    "df_top_brands.to_parquet(out_path, index=False)\n",
    "print(f\"[done] wrote {out_path.resolve()}\")\n",
    "\n",
    "# quick peek for Denmark\n",
    "print(\n",
    "    df_top_brands.query(\"country == 'Denmark'\")\n",
    "                    .sort_values(\"rank\")[[\"brand\",\"count\",\"rank\"]]\n",
    "                    .to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366011",
   "metadata": {},
   "source": [
    "Each customer is counted once, in a mutually exclusive bucket based on the time from their first purchase to their first return on a different date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ced722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Return Buckets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def bucket_return_days(d: int) -> str | pd.NA:\n",
    "    if pd.isna(d) or d <= 0:          return pd.NA          # no return or same-day only\n",
    "    if 1 <= d <= 7:                   return \"week 1\"\n",
    "    if 8 <= d <= 14:                  return \"week 2\"\n",
    "    if 15 <= d <= 21:                 return \"week 3\"\n",
    "    if 22 <= d <= 30:                 return \"1 month\"\n",
    "    # months as 30-day blocks up to 12 months\n",
    "    for m in range(2, 13):            # 2..12 months\n",
    "        lo, hi = 30*(m-1)+1, 30*m     # e.g., 31-60, 61-90, ...\n",
    "        if lo <= d <= hi:             return f\"{m} months\"\n",
    "    if d > 365:                       return \"> 1 year\"\n",
    "    return pd.NA\n",
    "\n",
    "def count_return_buckets(df_items: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_items.copy()\n",
    "    df[\"created\"] = pd.to_datetime(df[\"created\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"created\", \"customer_id\"])\n",
    "    df[\"purchase_date\"] = df[\"created\"].dt.date\n",
    "\n",
    "    # unique purchase dates per customer\n",
    "    uniq = df[[\"customer_id\",\"purchase_date\"]].drop_duplicates()\n",
    "\n",
    "    # first purchase per customer\n",
    "    first_date = uniq.groupby(\"customer_id\")[\"purchase_date\"].min().rename(\"first_date\")\n",
    "\n",
    "    # earliest purchase strictly AFTER first_date → first return date\n",
    "    tmp = uniq.merge(first_date, on=\"customer_id\")\n",
    "    tmp = tmp[tmp[\"purchase_date\"] > tmp[\"first_date\"]]\n",
    "    first_return = tmp.groupby(\"customer_id\")[\"purchase_date\"].min().rename(\"first_return_date\")\n",
    "\n",
    "    # join and compute delta days\n",
    "    timeline = first_date.to_frame().merge(first_return, left_index=True, right_index=True, how=\"left\")\n",
    "    timeline[\"days_to_return\"] = (pd.to_datetime(timeline[\"first_return_date\"]) - \n",
    "                                  pd.to_datetime(timeline[\"first_date\"])).dt.days\n",
    "\n",
    "    # bucketize (mutually exclusive)\n",
    "    timeline[\"bucket\"] = timeline[\"days_to_return\"].apply(bucket_return_days)\n",
    "\n",
    "    # keep only customers who returned (bucket not NA)\n",
    "    ret = timeline.dropna(subset=[\"bucket\"])\n",
    "\n",
    "    # counts by bucket\n",
    "    counts = (ret.groupby(\"bucket\").size().reset_index(name=\"customers\")\n",
    "                .sort_values(\"customers\", ascending=False)\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "    # optional: enforce a readable bucket order\n",
    "    order = ([\"week 1\",\"week 2\",\"week 3\",\"1 month\"] +\n",
    "             [f\"{m} months\" for m in range(2,13)] + [\"> 1 year\"])\n",
    "    counts[\"order\"] = counts[\"bucket\"].map({b:i for i,b in enumerate(order)})\n",
    "    counts = counts.sort_values(\"order\").drop(columns=\"order\").reset_index(drop=True)\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c71fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bucket  customers\n",
      "0      week 1       1284\n",
      "1      week 2       1715\n",
      "2      week 3       1219\n",
      "3     1 month       1262\n",
      "4    2 months       3047\n",
      "5    3 months       2444\n",
      "6    4 months       2188\n",
      "7    5 months       1769\n",
      "8    6 months       1994\n",
      "9    7 months       1640\n",
      "10   8 months       1286\n",
      "11   9 months        890\n",
      "12  10 months        722\n",
      "13  11 months        606\n",
      "14  12 months        464\n",
      "15   > 1 year        586\n"
     ]
    }
   ],
   "source": [
    "# df_items = pd.read_parquet(\"data/order_items.parquet\")\n",
    "df_buckets = count_return_buckets(df_items)\n",
    "print(df_buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66661cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buckets.to_parquet(\"../data/parquet_out/return_buckets_overall.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a56d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (container)",
   "language": "python",
   "name": "py310-vscode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
